[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Cateloging my machine learning adventures."
  },
  {
    "objectID": "posts/palmerPenguins/index.html#abstract",
    "href": "posts/palmerPenguins/index.html#abstract",
    "title": "Palmer Penguins!",
    "section": "Abstract",
    "text": "Abstract\nWe investigate the Palmer Penguin data set. We investigate quantitative and qualitative features that partion the set by species. We confirm our results with an automated feature selection process, and test four different model accuracy and speed (Decision Trees, Random Forrests, K-Nearest Neighbors, Logistic Regression)."
  },
  {
    "objectID": "posts/palmerPenguins/index.html#exploring-the-dataset",
    "href": "posts/palmerPenguins/index.html#exploring-the-dataset",
    "title": "Palmer Penguins!",
    "section": "Exploring the Dataset",
    "text": "Exploring the Dataset\nWe’ll first start by exploring the data set. I am most curious how different features partition the penguins by species. This will lend some context later on when we train our model.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove are box and wisker plots for penguin body mass and flipper length subdevided by species. Interesting, both Chinstrap and Adelie Penguins have similar distriubtions of these features. However, weight and flipper length clearly distinguishes Gentoo penguins from their counterparts. Moreover, flipper length appears to have a tighter spread accross Gentoo and Adelie penguins. Speaking of penguins, here’s the squad:\n\n\n\n\n\n\n\n\n\nChinstrap\n\n\n\n\n\n\n\nGentoo\n\n\n\n\n\n\n\nAdelie\n\n\n\n\n\nAs a certified penguin enjoyer, I also couldn’t help but notice that these penguins appear to have different types of beaks. The two features that correspond to this information are Culmen Length and Culmen depth. If we expect the beaks to be a distingishing feature, we might expect some seperation in the data set when we plot the two against eachother.\n\nWow! We are able to clearly distinguish each species of penguin by the ratio of their culmen length to culmen depth. This might suggest that among these three penguins within this region, beaks have adapted to suit each individuals speice’s unique needs like hunting, preening, and defense.\nI’m also curious about the species demographics accross the three islands.\n\nInterestingly, Adelie penguins live on all three islands, and are the only species residing on Torgersen island. Dream Island also contains Chinstrap Penguins, and Biscoe Island contains the entire Gentoo population.\nFinally, we’ll take a look at a pairplot for select quantiative features to see if there is anything else that might be useful:\n\nAlong the main diagonal, we can see the distriubtion of the features themselves for the three different species populations. We see that each feature only seperates out certain species. For example Flipper Length has one distinct mound centered right, but the two other mounds are overlapping. Besides culmen length to culmen depth, two other noteworthy charts our culmen length to flipper length, and culmen length to body mass."
  },
  {
    "objectID": "posts/palmerPenguins/index.html#feature-selection",
    "href": "posts/palmerPenguins/index.html#feature-selection",
    "title": "Palmer Penguins!",
    "section": "Feature Selection",
    "text": "Feature Selection\nThe intial exploration of the data suggestions that if we are clever about feature selection, we should be able to select features the cut the dataset by penguin species. To confirm this, we will use recursive features selection (REF). REF works by assigning a predictive socre to each feature. In this case, we’ll use linear regression. It then eliminates low-scoring features. The process is repeated until the desired number of features is reached. We’ll define our REF function as follows:\n\ndef select_features(df, target_column, n) :\n    #returns n most predictive features for target_column\n    x = df.drop(target_column, axis = 1)\n    y = df[target_column]\n    [x_train, x_test, y_train, y_test] = train_test_split(x, y, test_size = 0.2, random_state = 42)\n    LR = LinearRegression()\n    rfe = RFE(estimator = LR, n_features_to_select = n)\n    rfe.fit(x_train, y_train)\n    return x.columns[rfe.support_]\n\nTo make things interesting, we’ll use two quantitative features and one qualitative feature. To do this, we’ll subdevide the dataset by quantitative features and qualitative features, and pass the augmented dataframe to our select features_features functions. In each case, we’ll look at the top 3 features:\n\n#Defining Cuts\nquantitativeFeatures = [\"Species\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\",  \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nqualitativeFeatures = [\"Species\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]\n#Selecting Features\nquantFeatures = utility.select_features(penguins[quantitativeFeatures], \"Species\", 3)\nqualFeatures = utility.select_features(penguins[qualitativeFeatures], \"Species\", 3)\n\nThe three best quantiative features are Culmen Length, Culmen Depth, and Delta 15 N. The three best qualitative features are Island_Biscoe, Island_Torgenrsen, and Clutch Completion. This confirms our suspicious in our initial analysis. For the quantitative features, we’ll take Culmen Length and Culmen Depth. In the intial data cleaning, I actually partitioned Island into three seperate columns, so we’ll just take Island as our single qualitative feature."
  },
  {
    "objectID": "posts/palmerPenguins/index.html#model-training",
    "href": "posts/palmerPenguins/index.html#model-training",
    "title": "Palmer Penguins!",
    "section": "Model Training",
    "text": "Model Training\n\nGiven the nature of this data set, there’s a high likelilhood we will be able to train a model with 100% accuracy. A better question to ask here is what types of models work well with the features that we choose. From our initial gander at the data, we saw that no single feature cleanly partitions the penguins by species. However, composite features like Culmen Length to Column depth appeared to have high predictive power. Regression might then be a good choice here; particulary, we’ll take a look at logistic regression.\nAnother important observation is that features can reduce the number of choices that the model has to make. For example, if a model is passed Torgersen Island, there is an 100% chance that it is looking at a Adelie penguin. Even if the model is passed Biscoe Island, it still narrows the number of choices down to two birds (Adelie and Gentoo). From here, other features can be used to classfy. This type of logic implicates decision trees and random forrests. We’ll test these as well.\nWe’ll define our predictive features and train the models as follows:\n\n#Defining features\npredictiveFeatures = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n#Initializing Models\ntree = DecisionTreeClassifier() #1\nRF = RandomForestClassifier() #2\nKN = KNeighborsClassifier() #3\nLR = LogisticRegression(max_iter = 1000000) #4\nmodels = [tree, RF, KN, LR]\nmodelNames = [\"Decision Tree\", \"Random Forrest\", \"K - Nearest Neighbors\", \"Logistic Regress\"]\na = []\ni = 0\n#Training and Testing\nfor model in models:\n\n    model.fit(penguins[predictiveFeatures], targetFeature)\n    pred = model.predict(testingPenguins[predictiveFeatures])\n    accuracy = accuracy_score(pred, testTargetFeature)\n    print(modelNames[i] + \" Accuracy: \" + str(accuracy))\n    i += 1\n\nThe accuracies on the testing set are as follows: The Decision Tree scored 98.5%, the Random Forrest 100%, K - Nearest Neighbors 98.5%, and Logistic Regress 100%. We can investigate the decision process of our model by taking a look at the decision boundaries and confusion matrix:\n\n\n\n\n\n\n\n\n\nDecision Boundaries\n\n\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\nThe decision tree made one wrong classification, mistaking a Adelie penguin for a Gentoo. Examining the decision boundaries, we see that a majority of the penguins are easily partitioned into their respective species, with the exception for a few penguins which lie near the boundaries. Interesting, although Torgersen island only contained Gentoo penguins, the model seemed to expect that other species could live there.\n\n\n\n\n\n\n\n\n\nDecision Boundary\n\n\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\nThe Random Forrest model correctly classified all the penguins. Unlike the Decision Tree Model, the Random Forrest was able to find that certain islands only contain certain species of penguins. Boundaries are also more linear, distinctlly partitioning islands into two decision regions.\n\n\n\n\n\n\n\n\n\nDecision Boundary\n\n\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\nThe K-Nearest Neighbors Model mistook a Gentoo penguin for a Chinstrap penguin. Similar to the Decision Tree, the model assumed that all species of penguins may be present on each island. K-Nearest Neights also has “rough” decision boundaries, which likely reflects the mechanics of the model.\n\n\n\n\n\n\n\n\n\nDecision Boundary\n\n\n\n\n\n\n\nConfusion Matrix\n\n\n\n\n\nLogistic Regression was able to correctly classify all penguins. Out of all the models, Logistic Regression appears to have the cleanest decision boundaries, with no points close to boundary. Finally, it is worth remarking that K-Nearest Neighbors and Decision Tree were able to train and predict on the data the fastest. This was followed by Logistic Regression and then the Random Forrest. This means that Logistic Regression is the fastest, accurate model."
  },
  {
    "objectID": "posts/palmerPenguins/index.html#discussion",
    "href": "posts/palmerPenguins/index.html#discussion",
    "title": "Palmer Penguins!",
    "section": "Discussion",
    "text": "Discussion\nOverall, our investigation suggests the a small number of features can produce high efficacy–If we are able to cleverly partition the data set. For toy data sets, automated methods such as recursive feature selection can aid in the feature selection process. Logistic Regression was the fastest, most accurate model. However, K-Nearest Neighbors and Decision Trees both produced a high degree of accuracy at about \\(\\frac{1}{6}^{th}\\) the speed. This makes them good options for large data sets (think billions of penguins) where we are O.K. with sacrificing accuracy for a boost in computation complexity. Random Forrests–although by far the slowest method–correctly classified all the penguins. This suggets that in more complex data sets where multiple features may be regquired to accuriately classify, Random Forrests can be a robust classifcation method."
  },
  {
    "objectID": "posts/logisticRegression/notebook.html",
    "href": "posts/logisticRegression/notebook.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "#Packages and Style\n%load_ext autoreload\n%autoreload 2\nimport torch\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nfrom logistic import LogisticRegression\nfrom logistic import Utility\nplt.rcParams['font.family'] = \"Courier\""
  },
  {
    "objectID": "posts/logisticRegression/notebook.html#abstract",
    "href": "posts/logisticRegression/notebook.html#abstract",
    "title": "Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nWe explore the mathematics that inform regression models and implement Logistic Regression in PyTorch. We perform tests to assess the correctness, speed, and generializability of the model. The code may be found at https://github.com/a1kyritsis/a1kyritsis_blog.github.io/tree/main/posts/logisticRegression."
  },
  {
    "objectID": "posts/logisticRegression/notebook.html#implementation",
    "href": "posts/logisticRegression/notebook.html#implementation",
    "title": "Logistic Regression",
    "section": "Implementation",
    "text": "Implementation\nWe walk through some of the important code chunks in our Logistic Regression algorithm. First, we score by taking the row-wise dot product, equivalent to matrix multiplication. If no weight vector is given, we initialize it:\n\n def score(self, X):\n        if self.w is None:\n            self.w = torch.rand((X.size()[1]))\n        return torch.matmul(X, self.w)\n\nNext, we define a prediction function. If the score of observation \\(s_i\\) satisfies \\(s_i &gt; 0\\), then we have found a seperating hyperplane and correctly classified the data. We achieve this as follows:\n\ndef predict(self, X):\n        score_vector = self.score(X)\n        return (score_vector &gt; 0).int()\n\nWe move to larger code blocks. We calculate the gradient using the mentioned formula\n\ndef grad(self, X, y, s):\n        #computes the gradient\n        n = X.shape[0]\n        return (1/n) * torch.sum((X * (self.sigmoid(s) - y).unsqueeze(1)), dim = 0)\n\nWe then implement a gradient descent optimizer, which performs the update \\(w_{k + 1} \\longleftarrow w_k - \\alpha \\nabla L(w_k) + \\beta (w_k - w_{k - 1})\\):\n\n def gradientDescentOptimizer(self, X, y, alpha, beta):\n        #spicy gradient descent\n        if self.w_prev == None:\n            self.w_prev = torch.zeros(X.shape[1]) # w_{k - 1}\n        \n        if self.w_next == None:\n            self.w_next = torch.zeros(X.shape[1]) #w_{k + 1}\n        \n        s = self.score(X) #initialize score for w\n        self.w_next = self.w - alpha * self.grad(X, y, s) + beta * (self.w - self.w_prev) #update\n        self.w_prev = self.w #reassign\n        self.w = self.w_next\n\nHere, \\(\\texttt{self.W} = w_k\\), \\(\\texttt{w\\_prev} = w_{k - 1}\\), and \\(\\texttt{w\\_prev} = w_{k + 1}\\). Finall for convenience, we also implement a training method that performs the looping functionality; or, the descent itself. The method records the number of iterations to converge and loss at each iteration.\n\ndef train_logistic(X, y, alpha, beta, epsilon, LR):\n        \n        L = LR.loss(X, y)\n        loss = []\n        step = 0\n\n        while(torch.norm(LR.grad(X, y, LR.score(X))) &gt; epsilon):\n            LR.gradientDescentOptimizer(X, y, alpha, beta)\n            L = LR.loss(X, y)\n            loss.append(L)\n            step += 1\n\n        return [step, loss]\n\nThe condition in the \\(\\texttt{while}\\) loop is the stopping condition. Recall that our loss is minimized when \\(\\nabla L = 0\\). In practice, this can be difficult to achieve, so we cheat by letting the gradient get close to \\(0\\), say some small positive constant \\(\\epsilon\\). This enables us to tune both model accuracy and speed of convergence."
  },
  {
    "objectID": "posts/logisticRegression/notebook.html#experiments",
    "href": "posts/logisticRegression/notebook.html#experiments",
    "title": "Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\n\nVanilla Gradient Descent\nWe first test a normal instance of gradient descent. Here, there is no momentum term so we take \\(\\beta = 0\\). We set \\(\\alpha = .5\\). For easy visualization, we set the number of features \\(p = 2\\), and the number of observations \\(n = 100\\). We also want to test if our algorithm is working on some easy data, so we will set the noise of the points \\(\\gamma = .12\\). Finally, we set the \\(\\epsilon\\)-tolerence to \\(.01\\).\n\n#Define parameters, testing data, and initialize model\nn = 100\np = 2\ngamma = .12\nX, y = Utility.classification_data(n, gamma, p)\nLR = LogisticRegression()\nLR.w = None\nalpha = .5\nbeta = .25\nepsilon = .01\n#Train the model and extract the weights\nres = Utility.train_logistic(X, y, alpha, beta, epsilon, LR)\nw = LR.w\n#plotting decision boundary\nplt.scatter(X[:,0], X[:, 1], c = y, cmap = \"coolwarm\")\nUtility.draw_line(w, -2, 2.5, plt, color = \"red\", linestyle = \"dashed\", label = \"Decision Boundary\")\nplt.legend()\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.title(\"Test Points\")\nplt.show()\n#plotting loss over iterations\n[num_iters, loss] = res\nprint(loss)\ni = torch.arange(0, num_iters)\nprint(loss) \nplt.plot(i, loss, color = \"red\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Model Loss Over Iterations\")\n\n\n\n\n\n\n\n\n[tensor(0.6644), tensor(0.6233), tensor(0.5773), tensor(0.5348), tensor(0.4970), tensor(0.4638), tensor(0.4344), tensor(0.4082), tensor(0.3847), tensor(0.3636), tensor(0.3445), tensor(0.3271), tensor(0.3112), tensor(0.2967), tensor(0.2833), tensor(0.2710), tensor(0.2597), tensor(0.2492), tensor(0.2395), tensor(0.2304), tensor(0.2220), tensor(0.2142), tensor(0.2068), tensor(0.1999), tensor(0.1935), tensor(0.1874), tensor(0.1817), tensor(0.1763), tensor(0.1712), tensor(0.1664), tensor(0.1619), tensor(0.1576), tensor(0.1535), tensor(0.1496), tensor(0.1459), tensor(0.1424), tensor(0.1390), tensor(0.1358), tensor(0.1327), tensor(0.1298), tensor(0.1270), tensor(0.1243), tensor(0.1217), tensor(0.1192), tensor(0.1169), tensor(0.1146), tensor(0.1124), tensor(0.1103), tensor(0.1083), tensor(0.1063), tensor(0.1044), tensor(0.1026), tensor(0.1008), tensor(0.0991), tensor(0.0975), tensor(0.0959), tensor(0.0944), tensor(0.0929), tensor(0.0914), tensor(0.0900), tensor(0.0887), tensor(0.0874), tensor(0.0861), tensor(0.0849), tensor(0.0837), tensor(0.0825), tensor(0.0814), tensor(0.0803), tensor(0.0792), tensor(0.0781), tensor(0.0771), tensor(0.0761), tensor(0.0752), tensor(0.0742), tensor(0.0733), tensor(0.0724), tensor(0.0715), tensor(0.0707), tensor(0.0699), tensor(0.0691), tensor(0.0683), tensor(0.0675), tensor(0.0667), tensor(0.0660), tensor(0.0653), tensor(0.0646), tensor(0.0639), tensor(0.0632), tensor(0.0626), tensor(0.0619), tensor(0.0613), tensor(0.0607), tensor(0.0600), tensor(0.0595), tensor(0.0589), tensor(0.0583), tensor(0.0577), tensor(0.0572), tensor(0.0567), tensor(0.0561), tensor(0.0556), tensor(0.0551), tensor(0.0546), tensor(0.0541), tensor(0.0536), tensor(0.0532), tensor(0.0527), tensor(0.0522), tensor(0.0518), tensor(0.0514), tensor(0.0509), tensor(0.0505), tensor(0.0501), tensor(0.0497), tensor(0.0493), tensor(0.0489), tensor(0.0485), tensor(0.0481), tensor(0.0477), tensor(0.0474), tensor(0.0470), tensor(0.0466), tensor(0.0463), tensor(0.0459), tensor(0.0456), tensor(0.0453), tensor(0.0449), tensor(0.0446), tensor(0.0443), tensor(0.0440), tensor(0.0437), tensor(0.0433), tensor(0.0430), tensor(0.0427), tensor(0.0424), tensor(0.0422), tensor(0.0419), tensor(0.0416), tensor(0.0413), tensor(0.0410), tensor(0.0408), tensor(0.0405), tensor(0.0402), tensor(0.0400), tensor(0.0397), tensor(0.0395), tensor(0.0392), tensor(0.0390), tensor(0.0387), tensor(0.0385), tensor(0.0382), tensor(0.0380), tensor(0.0378), tensor(0.0376), tensor(0.0373), tensor(0.0371), tensor(0.0369), tensor(0.0367), tensor(0.0365), tensor(0.0362), tensor(0.0360), tensor(0.0358), tensor(0.0356), tensor(0.0354), tensor(0.0352), tensor(0.0350), tensor(0.0348), tensor(0.0346), tensor(0.0344), tensor(0.0343), tensor(0.0341), tensor(0.0339), tensor(0.0337), tensor(0.0335), tensor(0.0333), tensor(0.0332), tensor(0.0330), tensor(0.0328), tensor(0.0327), tensor(0.0325), tensor(0.0323), tensor(0.0322), tensor(0.0320), tensor(0.0318), tensor(0.0317), tensor(0.0315), tensor(0.0314), tensor(0.0312), tensor(0.0310), tensor(0.0309), tensor(0.0307), tensor(0.0306), tensor(0.0305), tensor(0.0303), tensor(0.0302), tensor(0.0300), tensor(0.0299), tensor(0.0297), tensor(0.0296), tensor(0.0295), tensor(0.0293), tensor(0.0292), tensor(0.0291), tensor(0.0289), tensor(0.0288), tensor(0.0287), tensor(0.0285), tensor(0.0284), tensor(0.0283), tensor(0.0282), tensor(0.0280), tensor(0.0279), tensor(0.0278), tensor(0.0277), tensor(0.0276), tensor(0.0274), tensor(0.0273), tensor(0.0272), tensor(0.0271), tensor(0.0270), tensor(0.0269), tensor(0.0268), tensor(0.0266), tensor(0.0265), tensor(0.0264), tensor(0.0263), tensor(0.0262), tensor(0.0261), tensor(0.0260), tensor(0.0259), tensor(0.0258), tensor(0.0257), tensor(0.0256), tensor(0.0255), tensor(0.0254), tensor(0.0253), tensor(0.0252), tensor(0.0251), tensor(0.0250), tensor(0.0249), tensor(0.0248), tensor(0.0247), tensor(0.0246), tensor(0.0245), tensor(0.0244), tensor(0.0243), tensor(0.0242), tensor(0.0242), tensor(0.0241), tensor(0.0240), tensor(0.0239), tensor(0.0238), tensor(0.0237), tensor(0.0236), tensor(0.0235), tensor(0.0235), tensor(0.0234), tensor(0.0233), tensor(0.0232), tensor(0.0231), tensor(0.0230), tensor(0.0230), tensor(0.0229), tensor(0.0228), tensor(0.0227), tensor(0.0226), tensor(0.0226), tensor(0.0225), tensor(0.0224), tensor(0.0223), tensor(0.0223), tensor(0.0222), tensor(0.0221), tensor(0.0220), tensor(0.0220), tensor(0.0219), tensor(0.0218), tensor(0.0217), tensor(0.0217), tensor(0.0216), tensor(0.0215), tensor(0.0215), tensor(0.0214), tensor(0.0213), tensor(0.0212), tensor(0.0212), tensor(0.0211), tensor(0.0210), tensor(0.0210)]\n[tensor(0.6644), tensor(0.6233), tensor(0.5773), tensor(0.5348), tensor(0.4970), tensor(0.4638), tensor(0.4344), tensor(0.4082), tensor(0.3847), tensor(0.3636), tensor(0.3445), tensor(0.3271), tensor(0.3112), tensor(0.2967), tensor(0.2833), tensor(0.2710), tensor(0.2597), tensor(0.2492), tensor(0.2395), tensor(0.2304), tensor(0.2220), tensor(0.2142), tensor(0.2068), tensor(0.1999), tensor(0.1935), tensor(0.1874), tensor(0.1817), tensor(0.1763), tensor(0.1712), tensor(0.1664), tensor(0.1619), tensor(0.1576), tensor(0.1535), tensor(0.1496), tensor(0.1459), tensor(0.1424), tensor(0.1390), tensor(0.1358), tensor(0.1327), tensor(0.1298), tensor(0.1270), tensor(0.1243), tensor(0.1217), tensor(0.1192), tensor(0.1169), tensor(0.1146), tensor(0.1124), tensor(0.1103), tensor(0.1083), tensor(0.1063), tensor(0.1044), tensor(0.1026), tensor(0.1008), tensor(0.0991), tensor(0.0975), tensor(0.0959), tensor(0.0944), tensor(0.0929), tensor(0.0914), tensor(0.0900), tensor(0.0887), tensor(0.0874), tensor(0.0861), tensor(0.0849), tensor(0.0837), tensor(0.0825), tensor(0.0814), tensor(0.0803), tensor(0.0792), tensor(0.0781), tensor(0.0771), tensor(0.0761), tensor(0.0752), tensor(0.0742), tensor(0.0733), tensor(0.0724), tensor(0.0715), tensor(0.0707), tensor(0.0699), tensor(0.0691), tensor(0.0683), tensor(0.0675), tensor(0.0667), tensor(0.0660), tensor(0.0653), tensor(0.0646), tensor(0.0639), tensor(0.0632), tensor(0.0626), tensor(0.0619), tensor(0.0613), tensor(0.0607), tensor(0.0600), tensor(0.0595), tensor(0.0589), tensor(0.0583), tensor(0.0577), tensor(0.0572), tensor(0.0567), tensor(0.0561), tensor(0.0556), tensor(0.0551), tensor(0.0546), tensor(0.0541), tensor(0.0536), tensor(0.0532), tensor(0.0527), tensor(0.0522), tensor(0.0518), tensor(0.0514), tensor(0.0509), tensor(0.0505), tensor(0.0501), tensor(0.0497), tensor(0.0493), tensor(0.0489), tensor(0.0485), tensor(0.0481), tensor(0.0477), tensor(0.0474), tensor(0.0470), tensor(0.0466), tensor(0.0463), tensor(0.0459), tensor(0.0456), tensor(0.0453), tensor(0.0449), tensor(0.0446), tensor(0.0443), tensor(0.0440), tensor(0.0437), tensor(0.0433), tensor(0.0430), tensor(0.0427), tensor(0.0424), tensor(0.0422), tensor(0.0419), tensor(0.0416), tensor(0.0413), tensor(0.0410), tensor(0.0408), tensor(0.0405), tensor(0.0402), tensor(0.0400), tensor(0.0397), tensor(0.0395), tensor(0.0392), tensor(0.0390), tensor(0.0387), tensor(0.0385), tensor(0.0382), tensor(0.0380), tensor(0.0378), tensor(0.0376), tensor(0.0373), tensor(0.0371), tensor(0.0369), tensor(0.0367), tensor(0.0365), tensor(0.0362), tensor(0.0360), tensor(0.0358), tensor(0.0356), tensor(0.0354), tensor(0.0352), tensor(0.0350), tensor(0.0348), tensor(0.0346), tensor(0.0344), tensor(0.0343), tensor(0.0341), tensor(0.0339), tensor(0.0337), tensor(0.0335), tensor(0.0333), tensor(0.0332), tensor(0.0330), tensor(0.0328), tensor(0.0327), tensor(0.0325), tensor(0.0323), tensor(0.0322), tensor(0.0320), tensor(0.0318), tensor(0.0317), tensor(0.0315), tensor(0.0314), tensor(0.0312), tensor(0.0310), tensor(0.0309), tensor(0.0307), tensor(0.0306), tensor(0.0305), tensor(0.0303), tensor(0.0302), tensor(0.0300), tensor(0.0299), tensor(0.0297), tensor(0.0296), tensor(0.0295), tensor(0.0293), tensor(0.0292), tensor(0.0291), tensor(0.0289), tensor(0.0288), tensor(0.0287), tensor(0.0285), tensor(0.0284), tensor(0.0283), tensor(0.0282), tensor(0.0280), tensor(0.0279), tensor(0.0278), tensor(0.0277), tensor(0.0276), tensor(0.0274), tensor(0.0273), tensor(0.0272), tensor(0.0271), tensor(0.0270), tensor(0.0269), tensor(0.0268), tensor(0.0266), tensor(0.0265), tensor(0.0264), tensor(0.0263), tensor(0.0262), tensor(0.0261), tensor(0.0260), tensor(0.0259), tensor(0.0258), tensor(0.0257), tensor(0.0256), tensor(0.0255), tensor(0.0254), tensor(0.0253), tensor(0.0252), tensor(0.0251), tensor(0.0250), tensor(0.0249), tensor(0.0248), tensor(0.0247), tensor(0.0246), tensor(0.0245), tensor(0.0244), tensor(0.0243), tensor(0.0242), tensor(0.0242), tensor(0.0241), tensor(0.0240), tensor(0.0239), tensor(0.0238), tensor(0.0237), tensor(0.0236), tensor(0.0235), tensor(0.0235), tensor(0.0234), tensor(0.0233), tensor(0.0232), tensor(0.0231), tensor(0.0230), tensor(0.0230), tensor(0.0229), tensor(0.0228), tensor(0.0227), tensor(0.0226), tensor(0.0226), tensor(0.0225), tensor(0.0224), tensor(0.0223), tensor(0.0223), tensor(0.0222), tensor(0.0221), tensor(0.0220), tensor(0.0220), tensor(0.0219), tensor(0.0218), tensor(0.0217), tensor(0.0217), tensor(0.0216), tensor(0.0215), tensor(0.0215), tensor(0.0214), tensor(0.0213), tensor(0.0212), tensor(0.0212), tensor(0.0211), tensor(0.0210), tensor(0.0210)]\n\n\nText(0.5, 1.0, 'Model Loss Over Iterations')\n\n\n\n\n\n\n\n\n\nThe good news is that our model is able to cleanly partition the two points and loss is decreasing over time. This indicates that our model is working!\n\n\nBenefits of Momentum\nFor small data sets, rates of convergence it not a big issues. However, for \\(n\\) and \\(p\\) large (think billions) we would like to things to go fast. This is where the momentum term comes into play. Intuitvely, if two consecutive points both have large negative gradients, we likely need to increase our step size. If the converse is true, we ought to reduce it.\nAgain, we’ll let \\(n = 100\\). To make things harder, we will set \\(p = 10\\) and increase to \\(\\gamma = .25\\). We will also increase the \\(\\epsilon\\)-tolerence to \\(.1\\) for easier visualization. For Vanilla Gradient Descent, we again take \\(\\beta = 0\\) and change it to \\(\\beta = .75\\) for Gradient Descent with Momentum.\n\n#Define parameters and Testing Data\nn = 100\np = 10\ngamma = .25\nalpha = .1\nbeta = 0\nepsilon = .1\nX, y = Utility.classification_data(n, gamma, p)\n#Initialize Model and train Vanilla Gradient Descent\nLR = LogisticRegression()\nres = Utility.train_logistic(X, y, alpha, beta, epsilon, LR)\n#Reset and Train Gradient Descent with Momentum\nLR.w = None\nbeta = .75 #increase beta term\nres_momentum = Utility.train_logistic(X, y, alpha, beta, epsilon, LR)\n#plotting\n[num_iters, loss] = res\n[num_iters_momentum, loss_momentum] = res_momentum\niters = torch.arange(0, num_iters)\niters_momentum = torch.arange(0, num_iters_momentum)\nplt.plot(iters, loss, color = \"red\", label = \"Vanilla\")\nplt.plot(iters_momentum, loss_momentum, color = \"Blue\", label = \"Momentum\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Model Loss Over Iterations\")\nplt.legend()\n\n\n\n\n\n\n\n\nThis is a great demonstration. Not only does Momentum Gradient Descent converge in 40 iterations faster than Vanilla Gradient Descent, but we also see how the momentum term causes the algorithm to overestimate weights before converging to a more optimal solution!\n\n\nOverfitting\nFinally, we test overfitting. In machine learning, overfitting occurs when we choose features the produce a high degree of accuracy in the training data, but don’t generalize to testing data well. In our experiment, we can model this by letting the number of features be greater than the number of observation. We take \\(n = 15\\) and \\(p = 30\\) to improve the speed at which we can find a high accuracy solution. Here, we generate some test set which is unknown to the model. Then, we find a training set that enables the model to achieve 100% accuracy. Finally we extract the weights from the trained model, and apply them to test set to how well they generialize:\n\n#Define parameters, instantiate model\nbeta = .25\nn = 15\np = 30\ngamma = .5\nalpha = .1\nbeta = .1\nepsilon = .01\nLR = LogisticRegression()\n#generate training and testing data\nX_test, y_test = Utility.classification_data(n, gamma, p)\n#Generate new training sets until 100% accuracy is achieved\nwhile True:\n\n    X_train, y_train = Utility.classification_data(n, gamma, p)\n    res = Utility.train_logistic_with_test(X_train, y_train, X_test, y_test, alpha, beta, epsilon, LR)\n    pred = LR.predict(X_train)\n    matching_count = torch.sum(y_train == pred).item()\n    percentage_match = (matching_count / len(pred))\n\n    if (percentage_match == 1):\n        break\n    else:\n        LR.w = None\n#Printing Accuracy Results\nprint(\"Training Accuracy: \" + str(percentage_match))\npred = (torch.matmul(X_test, LR.w) &gt; 0).int()\nmatching_count = torch.sum(y_test == pred).item()\npercentage_match = (matching_count / len(pred))\nprint(\"Testing Accuracy: \" + str(percentage_match))\n#Loss over Iterations\n[num_iters, train_loss, test_loss] = res\niters = torch.arange(0, num_iters)\nplt.plot(iters, train_loss, color = \"blue\", label = \"Training Loss\")\nplt.plot(iters, test_loss, color = \"red\", label = \"Testing Loss\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Model Loss Over Iterations on Training and Testing Sets\")\nplt.legend()\n\nTraining Accuracy: 1.0\nTesting Accuracy: 0.8666666666666667\n\n\n\n\n\n\n\n\n\nUnsurprisingly, although our model has a training accuracy of 100%, the testing set only had a accuracy of 86%. Moreover, we see that as the model trains in the training set, it it does not reduce loss as dramatically on the testing set."
  },
  {
    "objectID": "posts/logisticRegression/notebook.html#conclusion",
    "href": "posts/logisticRegression/notebook.html#conclusion",
    "title": "Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\nGradient Descent in \\(\\mathbb{R}^2\\)\n\n\nToday we gave a high level overview of some of the mathematics the inform gradient descent. We used these ideas to implement gradient descent with Logistic Loss in PyTorch, discussing some of the components critical to the algorithm. Finally, we performed prooft-of-concept tests to demonstrate the mechanics behind regression models in the real world. Besides an intuitive understanding of gradient descent and loss minimization, there are two key observations that the reader should walk away with. First, gradient descent with momentum can speed up convergence, but may come at the cost of precision. As we saw in the second experiment, although Momentum Gradient Descent converged quicker, it first had to diverge. This can mean that It may “skip” or “bounce” over solutions more often than Vanilla Gradient Descent. Second, is that using too many features comes at the cost of algorithm speed and generializability. As we saw in the third experiment, although a large number of features may produce a high degree of accuracy on the training set, that predictive power does not generialize well. This truely is an instance of quality over quantity."
  },
  {
    "objectID": "posts/NewtonMethod/index.html",
    "href": "posts/NewtonMethod/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Newton\n#metadata and libraries \n%load_ext autoreload\n%autoreload 2\nimport torch\nimport newton as N\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = \"Courier\""
  },
  {
    "objectID": "posts/NewtonMethod/index.html#abstract",
    "href": "posts/NewtonMethod/index.html#abstract",
    "title": "Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nWe explore the mathematics that inform Newton’s Method for convex optimization and implement it in PyTorch. We perform a theoretical complexity analysis and emperically test the model in a variety of cases. Our finding suggest that Newton’s Method is much weaker when compared to instances of Gradient Descent, but may have specefic use cases in pre-optmized models."
  },
  {
    "objectID": "posts/NewtonMethod/index.html#implementation",
    "href": "posts/NewtonMethod/index.html#implementation",
    "title": "Logistic Regression",
    "section": "Implementation",
    "text": "Implementation\nThe mechanics of Newton’s Method are identical to that of Gradient Descent with exception to the computation of the Hessian. We give a refresher of some of the key functions here, with special emphasis to the computation of the Hessian.\nScoring is implemented with the following function\n\ndef score(self, X):\n\n        if self.w is None:\n            self.w = torch.rand((X.size()[1])) #initialize random weight vector\n        \n        return torch.matmul(X, self.w) #returns an n x 1 vector (column)\n\nWe may calculate the gradient with\n\ndef grad(self, X, y, s):\n        #computes the gradient\n        n = X.shape[0]\n        return (1/n) * torch.sum((X * (self.sigmoid(s) - y).unsqueeze(1)), dim = 0) #calculate gradient\n\nNow for the new parts of code. For logistic loss, we may compute the Hessian according to the formula described above. The implementation is as follows:\n\ndef hessian(self, X, s):\n        #computes the Hessian of Logistic Loss Function\n        n = X.shape[0] #get the number of rows\n        D = torch.zeros(n, n) #initialize the diagonal matrix\n        S = self.sigmoid(s) * (1 - self.sigmoid(s)) #put the scores in the desired form\n        D.diagonal().copy_(S) #set the diagonal values\n        return X.t() @ D @ X # the hessian is found\n\nThe pivotal step of the Hessian functions is to calculate the diagonal matrix \\(X\\). To do so, we find the diagonals values as a \\(1 \\times p\\) torch vector and copy them into a \\(p \\times p\\) matrix of \\(0\\)’s. The Hessian is nothing but the matrix product of \\(X^T D X\\), so we return it.\nThe final component of Newton’s Method is the per-iteration weight vector update. This is achieved with two lines of code:\n\ndef NewtonOptimizer(self, X, y, alpha):\n        #per iteration update of Newton Methods\n        s = self.score(X)\n        self.w = self.w - alpha * torch.inverse(self.hessian(X, s)) @ self.grad(X, y, s)\n\nFinally for clarity, as in Logistic Regression, we define a \\(\\texttt{train\\_newton}\\) function which updates the model till the gradient is \\(\\epsilon\\)-tolerable (i.e. “close-enough” to the global minimum):\n\ndef train_newton(X, y, alpha, epsilon, NO):\n    #Trains Newton's Method\n    L = NO.loss(X, y)\n    loss = []\n    gradients = []\n    step = 0\n\n    while (torch.norm(NO.grad(X, y, NO.score(X))) &gt; epsilon):\n        \n        NO.NewtonOptimizer(X, y, alpha)\n        L = NO.loss(X, y)\n        print(L)\n        loss.append(L)\n        gradients.append(torch.norm(NO.grad(X, y, NO.score(X))))\n        step += 1\n    \n    return [step, loss, gradients]\n\nThe function returns the number of iterations to convergence, the per-iteration loss, and per-iteration gradient. We also implement a counterpart method \\(\\texttt{train\\_logistic}\\), which we leave to the imagination of the reader to decide what it does. The information these functions provide will be useful when we perform our emperical analysis of the algorithm’s performance."
  },
  {
    "objectID": "posts/NewtonMethod/index.html#complexity-analysis",
    "href": "posts/NewtonMethod/index.html#complexity-analysis",
    "title": "Logistic Regression",
    "section": "Complexity Analysis",
    "text": "Complexity Analysis\nAlthough Newton’s Method can converge faster when the weight vector \\(\\vec{w}\\) is close to the true solution, it is costly to compute. In addition to calculating the gradient, we must also compute the Hessian and its inverse. We make some simple assumptions to demonstrate the theoretical complexity of Newton’s Method relative to Gradient Descent.\nLet \\(c\\) be a computational unit. Assume it costs \\(c\\) units to compute loss \\(L\\), \\(2c\\) units to compute the gradient \\(\\nabla L\\), and \\(pc\\) units to compute the Hessian. Suppose also that it costs \\(k_1 p^{\\gamma}\\) units to invert a \\(p \\times p\\) matrix and \\(k_2 p^2\\) units to perform the matrix-vector multiplication by Newton’s Method. Finally, assume that Newton’s Method conveges to an \\(\\epsilon\\)-tolerable loss in \\(t_{nm}\\) iterations, while Gradient Descent converges to the same solution in \\(t_{gd}\\).\nFirst, we calculate per-iteration operation complexity for Newton’s Method. Since these operations are performed sequentially once per iteration, we sum them to see \\[\\begin{align*}\n\\qquad &\\text{Loss} + \\text{Gradient} + \\text{Hessian} + \\text{Inversion} + \\text{Matrix Multiplication} \\\\\n&= c + pc + 2c + k_1 p^{\\gamma} + k_2 p^2 \\\\\n&= 3c + pc + k_1 p^{\\gamma} + k_2 p^2\n\\end{align*}\\] Since we converge in \\(t_{nm}\\) iterations, it follows that total operational-complexity is given by\n\\[O\\left( t_{nm} \\left[ 3c + pc + k_1 p^{\\gamma} + k_2 p^2  \\right] \\right).\\]\nGradient Descent only requires calculation of the gradient and the loss function. We may apply similar logic to produce total operational complexity of\n\\[O\\left( t_{gd} \\cdot 3c \\right).\\]\nSince the cost of computation is fixed per iteration, for Newton’s Method to payoff, we need it to converge faster than Gradient Descent. To see by how much in terms of the computational complexity, we can set up and solve the following inequality: \\[\\begin{align*}\n    t_{gd} \\cdot 3c &\\ge t_{nm} \\left( 3c + pc + k_1 p^{\\gamma} + k_2 p^2  \\right) \\\\\n     t_{nm} &\\le t_{gd} \\left( \\frac{3c}{3c + pc + k_1 p^{\\gamma} + k_2 p^2} \\right).\n\\end{align*}\\]\nNotice that \\(c\\), \\(k_1\\), and \\(k_2\\) are constants determined by the processor. For matrix inversion, \\(\\gamma\\) satisfies \\(2 \\le \\gamma \\le 3\\). We take \\(\\gamma = 3\\) to for the worst-case, Big-\\(O\\) bound. The variable here is then \\(p\\), the number of features. Observe that for \\(p\\) large, the \\(k_1 p^{\\gamma}\\) term dominates. Hence, Newton’s method must converge at an inverse cubic rate to see any computational benefits.\nWe explore this idea in practice in the following section."
  },
  {
    "objectID": "posts/NewtonMethod/index.html#experiments",
    "href": "posts/NewtonMethod/index.html#experiments",
    "title": "Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\nWe will first test if our implementation is working. Here we’ll run Newton’s Method on \\(n = 100\\) points with noise \\(\\gamma = .25\\). In order to visualize the descision boundary in the plane, we’ll let the number of features \\(p = 2\\). For the input parameters, we’ll take learning rate \\(\\alpha = .3\\) and convergence tolerence \\(\\epsilon = .01\\).\n\nn = 100\ngamma = .25\np = 2\n[X, y] = N.Utility.classification_data(n, gamma, p)\nNO = N.NewtonMethod()\nalpha = .3\nepsilon = .01\n[steps, loss, grad] = N.Utility.train_newton(X, y, alpha, epsilon, NO)\nprint(\"done.\")\n#plotting decision boundary\nw = NO.w\nplt.scatter(X[:,0], X[:, 1], c = y, cmap = \"coolwarm\")\nN.Utility.draw_line(w, -2, 2.5, plt, color = \"red\", linestyle = \"dashed\", label = \"Decision Boundary\")\nplt.legend()\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.title(\"Test Points\")\nplt.show()\n#Loss over iterations\n#plotting loss over iterations\niterations = torch.arange(1, steps + 1)\nplt.plot(iterations, loss, color = \"red\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Model Loss Over Iterations\")\n\ndone.\n\n\n\n\n\n\n\n\n\nText(0.5, 1.0, 'Model Loss Over Iterations')\n\n\n\n\n\n\n\n\n\nOn the test points, we see that the algorithm is clearly able to distinguish between different groups. The loss over iterations is also monotonically decreasing. This both suggests that the algorith is up and runnin properly.\nWe’ll now compare Newtons Method to both Gradient Descent and Gradient Descent with Momentum. We’ll keep the learning rate parameter \\(\\alpha\\) the same for all tests, and introduce a small momentum term \\(\\beta = .3\\) for First, we will look at a situation where the weight vector \\(\\vec{w}\\) starts far from the target weights \\(\\vec{w}^*\\). We may simulate this with larger values of \\(n\\) and \\(p\\), since all models will have to perform more iterations to converge to \\(\\vec{w}^*\\):\n\n#Initialize Parameters\nn = 100\ngamma = .25\np = 25\nalpha_N = .2\nalpha_R = .2\nbeta = 0\nepsilon = .01\n[X, y] = N.Utility.classification_data(n, gamma, p)\nNO = N.NewtonMethod()\nLR = N.LogisticRegression()\nLRM = N.LogisticRegression()\n[N_steps, N_loss, grad] = N.Utility.train_newton(X, y, alpha_N, epsilon, NO) #train the Newton Method\n[L_steps, L_loss] = N.Utility.train_logistic(X, y, alpha_R, beta, epsilon, LR) #train Logistic Regression\nbeta = .3 #Momentum Term\n[LM_steps, LM_loss] = N.Utility.train_logistic(X, y, alpha_R, beta, epsilon, LRM) #Train Logistic Regression with Momentum\nN_iters = torch.arange(1, N_steps + 1)\nL_iters = torch.arange(1, L_steps + 1)\nLM_iters = torch.arange(1, LM_steps + 1)\nplt.plot(N_iters, N_loss, color = \"red\", label = \"Newton's Method\")\nplt.plot(L_iters, L_loss, color = \"blue\", label = \"Logistic Regression\")\nplt.plot(LM_iters, LM_loss, color = \"green\", label = \"Logistic Regression with Momentum\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Model Loss Over Iterations\")\nplt.legend()\n\n\n\n\n\n\n\n\nBoth instances of Gradient Descent outperformed Newton’s Method by north of 1000 iterations! Their loss curves are also steeper. Gradient Descient with Momentum converged the faster, and had the least amount of loss over the duration of its training cycle. Now we’ll look at the situation where \\(\\vec{w}\\) starts relatively closer to \\(\\vec{w}^*\\). We can simulate this with smaller values of \\(n\\), \\(p\\), and \\(\\gamma\\), say \\(50\\), \\(2\\) and \\(.125\\) respectively.\n\n#Initialize Parameters\nn = 50\ngamma = .25\np = 2\nalpha_N = .2\nalpha_R = .2\nbeta = 0\nepsilon = .01\n[X, y] = N.Utility.classification_data(n, gamma, p)\nNO = N.NewtonMethod()\nLR = N.LogisticRegression()\nLRM = N.LogisticRegression()\n[N_steps, N_loss, grad] = N.Utility.train_newton(X, y, alpha_N, epsilon, NO) #train the Newton Method\n[L_steps, L_loss] = N.Utility.train_logistic(X, y, alpha_R, beta, epsilon, LR) #train Logistic Regression\nbeta = .3 #Momentum Term\n[LM_steps, LM_loss] = N.Utility.train_logistic(X, y, alpha_R, beta, epsilon, LRM) #Train Logistic Regression with Momentum\nN_iters = torch.arange(1, N_steps + 1)\nL_iters = torch.arange(1, L_steps + 1)\nLM_iters = torch.arange(1, LM_steps + 1)\nplt.plot(N_iters, N_loss, color = \"red\", label = \"Newton's Method\")\nplt.plot(L_iters, L_loss, color = \"blue\", label = \"Logistic Regression\")\nplt.plot(LM_iters, LM_loss, color = \"green\", label = \"Logistic Regression with Momentum\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Model Loss Over Iterations\")\nplt.legend()\n\n\n\n\n\n\n\n\nIn this particular instance, Newton’s Method was able to converge faster than both Logistic Regression Models. On average, I found that Newton’s Method definitively converged faster than Vanilla Gradient Descent, and generially tended to tie with the Momentum model. Since we are generating random data and initial weight vectors this result is unsuprising. Even within testing groups, some point-vector combinations will be closer to their ideal solutions \\(\\vec{w}*\\) which enables Newton’s Method to converge faster.\nFinally, we’ll investigate Newton’s Method when the learning rate parameter \\(\\alpha\\) is large. We will test this on a linear and non-linear data sets, and compare the loss over iterations. The data sets have the following shapes respectively:\n\n#Input Parameters\nn = 100\ngamma = .5\np = 2\nr1 = 4\nr2 = 3\n#Linear Data\n[X_lin, y_lin] = N.Utility.classification_data(n, gamma, p)\n#Non Linear Data\np1 = N.Circle.generate_points(r1, n, gamma)\np2 = N.Circle.generate_points(r2, n, gamma)\nX = torch.cat((p1, p2), dim = 0)\ncol = 1.0 * torch.ones(2 * n, 1)\nX = torch.cat((X, col), dim = 1)\ny1 = 1.0 * torch.zeros(1, n)\ny2 = 1.0 * torch.ones(1, n)\ny_circ = torch.cat((y1, y2), dim = 1)[0, :]\nX_circ = X.float()\n#Plotting Data Linear Data\nplt.scatter(X_lin[:,0], X_lin[:, 1], c = y_lin, cmap = \"coolwarm\")\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.title(\"Linear Test Points\")\nplt.show()\n\n#Plotting Non-Linear Data\nfig = N.Circle.plot_points([p1, p2])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe’ll now instantiate the model\n\nalpha = 130\nepsilon = .1\nNO = N.NewtonMethod()\n[N_steps_l, N_loss_l, grad_l] = N.Utility.train_newton(X_lin, y_lin, alpha, epsilon, NO) #train the Newton Method\nNO.w = None\n[N_steps_c, N_loss_c, grad_c] = N.Utility.train_newton(X_circ, y_circ, alpha, epsilon, NO)\n#plotting loss over iterations\nN_iters_l = torch.arange(1, N_steps_l + 1)\nN_iters_c = torch.arange(1, N_steps_c + 1)\nprint(N_steps_l)\nplt.plot(N_iters_l, N_loss_l, color = \"red\", label = \"Linear Data\")\nplt.plot(N_iters_c, N_loss_c, color = \"blue\", label = \"Non-Linear Data\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Loss\")\nplt.title(\"Model Loss Over Iterations\")\nplt.legend()\nplt.show()\nplt.plot(N_iters_l, grad_l, color = \"red\", label = \"Linear Data\")\nplt.plot(N_iters_c, grad_c, color = \"blue\", label = \"Non-Linear Data\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Gradient Norm\")\nplt.title(\"Model Gradient Over Iterations\")\nplt.legend()\nplt.show()\n\n_LinAlgError: linalg.inv: The diagonal element 1 is zero, the inversion could not be completed because the input matrix is singular.\n\n\nSome analysis goes here."
  },
  {
    "objectID": "posts/NewtonMethod/index.html#conclusion",
    "href": "posts/NewtonMethod/index.html#conclusion",
    "title": "Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nToday, we explored Newton’s Method applied to gradient descent. We investigated some of the mathematical theory that informs this method, learning how to transform a zero-finding problem to an optimization problem. We extended this notion to the multivariant setting with the concept of the Hessian and Gradeint. Next, we investigated the computational complexity of running Newton’s Method relative to Gradient Descent. We emperically compared the convergence rate of these two methods and analyzed Newton’s Method on non-linear training data.\nOverall, our findings suggest that Newton’s Method is weaker than both instances of Gradient Descent. For any meaningful analysis (i.e. \\(p \\ge 2\\)), Newton’s Method requires the calculation and inversion of the Hessian matrix \\(\\mathcal{H}_L\\). This can be quite computationally intensive. Our theoretical analysis reveals that Newton’s Method must converge at an inverse cubic rate relative to Gradient Descent to see any performance benefits. However, in practice we observed that Newton’s Method is much slower when compared to Gradient Descent for generial analysis, and marginial quicker in specefic testing instances. Further, we also found that if \\(\\alpha\\) is not chosen carefully, Newton’s Method can have trouble converging on non-linear data.\nOur findings suggest that Gradient Descent (particularly with momentum) is a better generial-purpose optimization algorithm, as it has robust performance across multiple use cases. In cases where we believe our initial weight vector is close to the target, Newton’s Method can offer some performance benefits. One possible instances of this is if we have reason to believe certain input features to our model are more important than others. Another instance could be if we are switching one feature out for another. Here, we already have a partially trained weight vector that is close to the target, and the model may only need a small number of steps to achieve convergence.\nQuestions: - Is my operation counting correct? - Intuition behind NM correct? - Having trouble forcing divergence even with non-linear training data\nAnswers:\nconfirm that newtons method is indeed divergin:\n–&gt; do this with norm of weight vector and or loss output –&gt; for logistic regression, add a graph of loss for the training set and testing set (we can do this by graphing the loss over time for the testing set and training set)"
  },
  {
    "objectID": "posts/blog-template/index.html",
    "href": "posts/blog-template/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/blog-template/index.html#math",
    "href": "posts/blog-template/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/WomenInDataScience/index.html",
    "href": "posts/WomenInDataScience/index.html",
    "title": "Women in Data Science",
    "section": "",
    "text": "Abstract\nWomen participation in STEM fields is up across must subdivisions. However, computer science and computing related subdisciplines lag behind. This article surveys some of the social, historical, and economic forces that contribute to this phenomenon. We examine how positive female role models contribute to retention and engagement. We consider one such instance; namely, Middlebury’s Women in Data Science conference.\n\n\n\nMargaret Hamilton\n\n\n\n\nWomen in Data Science\nWomen in STEM fields have dramatically increased over the last 80 years. In 1960 women accounted for 8% of chemists. That number is now up to 39%. Over the same time period, women in biology increased from 25% to 50%. However, engineering fields have remained relatively stagnant, with women in computing related occupations declining since 1990. Currently, 1 in 5 computing degrees are awarded to women. Given that other STEM fields are approaching parity, what’s up with computer science?\nHistorically this was not the case. In the early 20th century, computing jobs were associated with secretarial and clerical positions, meaning they were often staffed by women. By WWII women made up the majority of programming jobs. In fact, women were some of the early trailblazers in computing: 6 female programmers worked on the first all-electronic computer; Grace Hopper designed, A-0, the first compiler; Margaret Hamilton engineered software for the Apollo Program, and Katherine Goble, Dorothy Vaughan, and Mary Jackson worked on calculations critical to the launch.\nHowever, the newly found prestige in the field of computer science actuated the decline in female participation. Increasing professionalization across the field meant a stronger connection to engineering; a male dominated industry due to its cultural and professional relations with heavy industry. One aspect in which this manifested was the creation of professional organizations, networks, and hierarchies that benefitted and encouraged male participation. Discriminatory hiring practices and stringent undergraduate admissions also put downward pressure on female participation. Perhaps most damning was the cultural association of the personal computer with boys and men. Primarily propagated through the gaming industry, this discouraged girls from pursuing computing careers at a young age. The complex matrix of cultural currents and historic events has left computing a male-dominated field.\nBut why do we even care? From a moral standpoint, it seems strange to construct social barriers that prevent individuals from entering a discipline based on uncontrollable traits. What if left-handed people were implicitly deterred from being doctors? Intuitively, we can agree that handedness ought not have a bearing on one’s qualifications to practice medicine. It is not a logical leap to say the same about gender in computer science. However, suppose you’re a pragmatist. Morals aside, the current system works, right? The problem is that when we culturally alienate half of the population from engineering and computing jobs, everyone misses out on the potential idea generation. This is particularly true in practical engineering and data science, where diverse backgrounds can often better inform application. Car seats were not initially designed with pregnant women in mind, leading to unnecessary deaths. Large data sets can benefit from the context of lived-experience. Perhaps most importantly, diversity can facilitate creativity and innovation: A culture of complacency breeds complacent ideas. Possessing a more accepting mindsend in relation to people can transfer to a more accepting one in relation to ideas.\nUnfortunately, increasing female participation in commuting is not as easy as inserting a column into a data set (although that itself can be tricky at times). Questions of acceptance and fairness raise themselves. How can we uphold meritocratic ideas when we clearly don’t live in one. Although laws are quick to change, culture and people are not. Perhaps one answer to this question is to showcase the work of outstanding female professionals in computing and computing-adjacent fields.\nWomen in engineering fields often describe feelings of isolation, lack of voice, and lack of belonging. Reports indicate that women find supervisors less receptive to their suggestions. They are less likely to agree that it is safe to speak up when compared to their male colleagues. Women suffer from a lack of other women in industry. This can make it harder for female computer scientists to find role models, and more likely to experience sexual harassment. Studies find that women are at increased risk of sexual discrimination in workplaces where they make up less than 25% of the labor force, and are less likely to speak up about these incidents to their male colleagues. Surveys also find women who left STEM careers were less likely to have training and development opportunities, support from coworkers or supervisors, and help balancing work and nonwork roles. Clearly, providing good role models and positive points of contact–namely in the form of other women–can create a more positive, safe work environment and increase recruitment and retention in the industry.\n\nOf course, this logic extends itself to undergraduate education as well. Middlebury’s Women in Data Science conference is a step towards creating a more equitable computing landscape–both inside and outside of the classroom. The conference also demonstrated the interdisciplinary nature of data science, showcasing data driven research from traditional ML specialists and computer scientists to field geographers and political scientists.\nDr. Amy Yuen started out the conference by asking Is the U.N. Security Democratic? The Security Council has five sitting members and 10 non-permanent members elected on an annual basis, and is one of the critical apparatuses for maintaining geopolitical stability. Sitting members possess veto powers and the ability to sponsor policies, whereas non-permanent only possess the latter. Given the power imbalances, Dr. Yuen turned to the data to find non-traditional avenues of power and metrics of representation. Her team constructed two theoretical council make-ups: One which represented a perfectly representative council, and the other a perfectly unrepresentative one. Using cumulative service year spent on the council per nation, and controlling for variables like council membership, Dr. Yuen was able to demonstrate that the council is mostly democratic.\nDr. Jessica L’Roe studies human-environment relationships in regions experiencing rapid change, and self-describes her work as “connecting people to pixels.” In the conference she presented her research in both Brazil and Africa. To prevent deforestation in Brazil, the government imposed an acreage cap on property size. Dr. L’Roe found that large landowners were registering under the size limits, explaining the increased rates of deforestation. In Africa, her team conducted a brute force, interpersonal survey of local property owners. Alongside two local women, they were able to conclude that external owners controlled much of the land, forcing local woodcutters to fell trees on restricted land. Dr. L’Roe also discussed the gender expectations associated with being a woman in science, and the importance of having positive, female role-models at critical points in her career.\nDr. Biester studies natural language processing where she hopes to better understand the human language through computational methods. Her recent work deals with manifestations of depression in text. Depression can be insidious in that depressed individuals may not even realize that they are displaying depressive tendencies. This can hamper effective treatment. To notice depression in its early stages, Dr. Biester developed a linear ML model to detect self-report patterns indicative of the underlying condition. To accomplish this, she turned to one of the internet’s largest sources of human text: reddit. Cleverly partitioning the text into control and test groups, she quite literally processed all of reddit, running calculations that lasted weeks.\nDr. Sarah Brown studies how data science can be integrated with human systems to improve outcomes. In her presentation, she discussed three “keys” that enabled her to find success in her work. Her first key arose in the context of developing a model to measure PTSD recovery progress. She realized that scores were only used to rank individuals who already possessed the condition, and were not scored relative to healthy individuals. The first key was then realizing that data resides in contexts. Her second experience dealt with the fundamentally incompatible notions of fairness: Error rate parity and calibration. She found that certain disciplines may be biased towards one definition, leading to the second key: disciplines are communities. Her final experience concerned work as a council member on the National Society of Black Engineers. She found that regional chapters were not accepting the program set by the national board. Ultimately, she discovered the disconnect was a restraint imposed by the universities through which the chapters were operating. Rather than pushing policy through, she realized the third key: meet people where they are.\nDr. Brown’s presentation also raised philosophical questions concerning notions of fairness and the scope of decision making within data science. However, Dr. Brown illustrated that fairness need not come at the expense of accuracy. She discussed one study which reviewed an algorithm for allocating social workers to people in need of medical care. The algorithm ultimately based decisions on who was in the most treatment programs; the logic here being that those individuals seeing more specialists would need more help scheduling treatment. The review found that the algorithm discriminated against black and brown individuals. In retrospect, this was obvious: Clearly, individuals who are seeing more doctors are likely to have time, money, and know-how to effectively navigate the healthcare system. The researchers proposed a revised algorithm that allocated social workers on the basis of symptoms, resulting in a far more equitable and accurate outcome.\nOne major reason women tend to leave computational jobs is a lack of work-related purpose. Academic and professional treatments of computation jobs can often exist in a vacuum of mathematical theory and computational mechanisms, without any clear context. Yet a common theme throughout all these presentations was the exact opposite: The use of context to construct experiments and meaningful interpret results. Theoreticians will always be important in driving the field of data science forwards. However, claiming that theory is the only way to do data science is as false as claiming that the domain is only for men.\n\n\nReflection\nI really enjoyed the presentations at the conference since they offered great insight into what type of research Middlebury (and other) professors are doing. The most interesting part of this blog post was researching some of the sociological forces that have contributed to a decline in women’s participation in computer science. I was particularly struck how “professionalization” of the field led to men taking a dominant role in the industry. I also never considered that living and/or working among a diverse population can lead to more diverse thinking. I am curious to learn more about contemporary thinking concerning the moral position of data science; particularly when data relates to people."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/index.html#introduction",
    "href": "posts/OptimalDecisionMaking/index.html#introduction",
    "title": "Optimal Decision Making",
    "section": "Introduction",
    "text": "Introduction\nThe primary way through which a bank makes money is a loan. The bank will lend you some sum of money which you then must pay back over the duration of the loan period, plus some interest. However, sometimes individuals may not be able to pay back their loan, causing them to default. Choosing who gets a loan is then a pivotal component of a bank’s profitability.\nToday, we will try to automate this process using a scoring function with a minimum threshold. We will design a profit maximizing algorithm using a sampling based approach, analyze its performance, and investigate its demographic impact. Although our model is profitable, ultimately we find that thresholded linear scoring methods can struggle to attain a high degree of accuracy, and hence profitability. We also find that input features to the model can result in a vicious cycle of discrimination."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/index.html#data-analysis",
    "href": "posts/OptimalDecisionMaking/index.html#data-analysis",
    "title": "Optimal Decision Making",
    "section": "Data Analysis",
    "text": "Data Analysis\nWe begin by taking a look at the data. First, let’s see what type of people are defaulting on their loans. While there are many features in the data frame that we might use to extrapolate on demographic information, there are three of interest. First, is loan intent, which is subdivided into Venture, Education, Medical, Home Improvement, Personal, and Debt Consolidation.\n\n\n\n\n\n\n\n\n\nRepaid\n\n\n\n\n\n\n\nDeafulted\n\n\n\n\n\n\nLoan Intent by Loan Status\n\n\n\nRunning a chi square test on the data returns Chi Square Statistic of approximately 368 with confidence value approaching 0. Clearly, whether or not someone defaults on their loans impacts the distribution of loan intent. However, a quick look at the charts reveals that the per-category difference is plus/minus a few percent. For our purposes, this does not indicate a clear bias, and we may confirm this in the correlation matrix: Indeed, the correlation between loan status and loan intent is .09. However, this information will be useful when assessing algorithm performance.\nNext, we’ll take a look at loan amount subdivided by defaults. This will be particularly useful when assessing algorithm profitability.\n\nA quick glance at the histogram shows that amount seems to have a negligible impact on defaults. Again, we may confirm this in the correlation matrix: loan amount has a .1 positive correlation with defaults. Finally, let’s look at home ownership. This is subdivided into those with mortgages, full home ownership, renters, and another category.\n\n\n\n\n\n\n\n\n\nRepaid\n\n\n\n\n\n\n\nDeafulted\n\n\n\n\n\nInterestingly, Renters comprise about 73% of individuals who defaulted on their loans but only 44% of those who repay them. We also see that, combined, home owners and individuals with mortgages make up about 26% of individuals who default, but 55% of individuals who repay their loans. We’ll also run a Chi Square Test on this feature, which results in a Chi-square statistic of approximately 1294 with confidence 2.913582391497053e-280! This makes qualitative sense: Individuals that own property are more likely to own assets, and be able to leverage said assets (home, financial, or otherwise) to repay loans. Homeownership also has a positive .22 correlation with loan status."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/index.html#feature-selection-and-model-training",
    "href": "posts/OptimalDecisionMaking/index.html#feature-selection-and-model-training",
    "title": "Optimal Decision Making",
    "section": "Feature Selection and Model Training",
    "text": "Feature Selection and Model Training\nThere are two clear candidate features to train on the model: Loan Percent of Income and Default on File. If one takes out a loan that is a greater percent of yearly income, it can be hard for them to pay it back along with other expenses. Moreover, if one defaults on their loan for some reason, we can assume that they are likely to do it again in the future. These features have positive correlation with loan status of .38 and .14 respectively.\nFrom our initial exploration, we also found that home ownership impacts loan repayment. How does it stack up against the loan’s percent of an individual’s income? For convenience analysis, call a loan amount that is greater than 50% of one’s income “high risk” and the converse “low risk”. We can then subdivide our dataset into high risk loans and low risk, and look at the frequency of homeownership type in each.\n\n\n\n\n\n\n\n\n\nLow Risk\n\n\n\n\n\n\n\nHigh Risk\n\n\n\n\n\n\nHomeownership Breakdown For Repaid and Defaulted Loans\n\n\n\nWe see that renters make up a larger portion of high risk loans. This observation lends some insight to our initial finding: Perhaps renters may also be more likely to default on their loans since they are more likely to take out loans that are a higher percent of their income.\nFor our model, we use logistic regression, and we’ll train on the features discussed (.e.g Loan Percent Income, Default on File, and Homeownership Type). This yields an accuracy of 84% on the testing data–well within industry standards and acceptable for a simple regression model."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/index.html#profit-optmization",
    "href": "posts/OptimalDecisionMaking/index.html#profit-optmization",
    "title": "Optimal Decision Making",
    "section": "Profit Optmization",
    "text": "Profit Optmization\nThis is where the fun begins. First, let’s define a simple profit model for the bank. If an individual is able to repay their loan, let’s assume they did so over a 10 year period. On the other hand, if an individual defaults on their loan, we’ll assume that this occurred after a 3 year period. We’ll also assume that the bank’s operating cost is 75% of the loan profit, to add some spice to our data (we don’t want to make things too easy). This leads to the following equations. Let \\(R\\) be the value of a repaid loan, \\(D\\) the value of a defaulted loan, \\(P\\) the principle, and \\(I\\) the decimal interest rate. Then we have\n\\[ R = P \\cdot (1 + I)^{10} - P \\] \\[D = P \\cdot (1 + I)^3 - P\\]\nWe’ll use these equations to create a profit and loss column – denoted PnL –in both our training and testing data. We can do this as follows:\n\n    creditTest[\"loan_amnt\"] * (1 +  percentProfit * .01 * creditTest[\"loan_int_rate\"])**10 - creditTest[\"loan_amnt\"] (REPAID)\n    creditTest[\"loan_amnt\"]* (1 +  percentProfit * .01 * creditTest[\"loan_int_rate\"])**3 - 1.7 * creditTest[\"loan_amnt\"] (DEFAULT)\n\nNext, we’ll define a theoretical portfolio of loans to benchmark our algorithm’s performance. We do this by adding a loan to our portfolio in the testing set if and only if the Profit and Loss is positive. The sum of these loans is the maximum possible profit the bank can make. We can find this as follows:\n\n    theoreticalPNL = creditTest.loc[creditTest['PnL'] &gt; 0, 'PnL'].sum()\n\nNow we need to find our threshold. Rather than use the actual weights from the model, a more elegant solution is to simply look at the probability score the regression assigns to each entry. We can add this column to both the training and test set:\n\n    credit[\"probability_score\"] = LR.predict_proba(credit[interestingFeatures])[:, 1]\n    creditTest[\"probability_score\"] = LR.predict_proba(creditTest[interestingFeatures])[:, 1]\n\nWith the stage set, we can now design a thresholding algorithm. Ideally, we would like to transform this to a a calc I optimization problem where we have profit \\(P\\) as a function of the threshold \\(T\\). To do this, we’ll discretize \\(T\\)’s domain by some constant \\(\\Delta\\). For each \\(T\\) value, we lend money if and only if the probability score is less than the threshold. We’ll calculate PnL for the loans made, and record it as a function of \\(T\\).\nThere are two problems with this approach. First, is time complexity. If \\(D\\) is the length of the data frame and \\(\\Delta\\) the discretization constant, This algorithm runs in \\(O(D \\cdot \\Delta)\\). While I can run this code on my 2016 macbook in under minutes, for larger datasets with more complex calculations, This is bad practice. A more pressing issue is overfitting. By taking the optima of a single sample (e.g. the training data), we may maximize for the sample but not for potential data in the population (e.g. the training set).\nTo remedy these issues, we’ll take \\(n\\) sample of size \\(k\\), for each of which we will record the \\(T\\) value that maximizes profit. We then define a new statistic, call it \\(T^*\\), which is the mean of the sample maximizes (Thanks central limit theorem!). Notice that \\(n \\cdot k &lt; D \\Longrightarrow n \\cdot k \\cdot \\Delta &lt; D \\cdot \\Delta\\), we can theoretically accomplish this more quickly then the aforementioned method. This method is also quite empirically accurate at threshold determination for \\(n,k\\) small. This is accomplished with the following functions:\n\ndef sampleMaxProfit(credit, n, k, scoreRange):\n    #samples size n from df credit.\n    #tests tresholds across \n    T = scoreRange[0]\n    maxTuple = [0, 0]\n    sample = credit.sample(n)\n    delta_T = (abs(scoreRange[1] - scoreRange[0]))/k\n    while T &lt; scoreRange[1]: #can we do this without a loop?\n        PnL = sample.loc[sample['probability_score'] &lt;= T, 'PnL'].sum()\n        if (PnL &gt; maxTuple[0]):\n            maxTuple = [PnL, T]\n        T += delta_T\n    return maxTuple[1] \n\ndef optimalProfitScore(credit, iterations, n, k):\n    #Finds treshold score T that optmizes profit\n    testResult = []\n    scoreRange = [credit[\"probability_score\"].min(), credit[\"probability_score\"].max()]\n    for i in range(0, iterations):\n        testResult.append(sampleMaxProfit(credit, n, k, scoreRange))\n    npTestResult = np.array(testResult)\n    return np.mean(npTestResult)\n\nThis produces the following result on the training set:\n\n\n\nProfit Curve\n\n\nNotice that our method (in orange) under estimates the optimal threshold value. This is because there is bias associated with our estimator. We can remedy this by multiplying by a proportionality constant. This works out to be about \\(1.05\\). The resulting curve now fits the training data better.\n\n\n\nCorrected Profit Curve\n\n\nThe threshold value that maximizes profit is 0.455 for the training set. Running the algorithm with 200 trials with sample size of 100 – which is less than the length of the data set at 22,000 – we receive a threshold of 0.445."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/index.html#evaluation---bank",
    "href": "posts/OptimalDecisionMaking/index.html#evaluation---bank",
    "title": "Optimal Decision Making",
    "section": "Evaluation - Bank",
    "text": "Evaluation - Bank\nTime to put our top hats on. First, for the features that we selected, we will see if the sampling method was able to obtain the optimal threshold.\n\n\n\nProfit Curve on Test Set\n\n\nThis looks good. Next we’ll compare the results of the algorithm against the theoretical loan portfolio. Our model has an accuracy of around 84%. The theoretical PnL is $12634399.24, the Model PnL is $7476722.014, and the Thresholded PnL is $7850920.35. Here Model PnL is the profit and loss if we use the model’s threshold. The Thresholded PnL is the profit and loss if we use our threshold. Using the thresholded PnL produces a 5% increase in profitability. However, both predictive methods capture just north of 60% of potential profits. While profit is not a linear function of accuracy, if we assigned loan amounts reflective of the population to the 80% of the data we predicted correctly, we might expect it to be closer to 80% of theoretical profits. This observation suggests two, not-necessarily-mutually-exclusive possibilities. First, the algorithm is not giving loans to high value loans. This would mean that we are leaving gainz on the table. Second, the algorithm is giving high value loans to people who are defaulting, which would put a downward pressure on profit. Let’s investigate.\nFirst, we’ll partition the data set into individuals selected for a loan and individuals whose loan request is rejected. Call these categories Algorithm-Select and Algorithm-Reject respectively. First, we consider defaults.\n\n\n\n\n\n\n\n\n\nSelection Default Loss\n\n\n\n\n\n\n\nReject Default Loss\n\n\n\n\n\n\nLoan Defaults for Alorithm Selections and Rejections\n\n\n\nOf those individuals who defaulted on their loans, the average loss for Algorithm-Select is $-5179. For the same default population, The average loss for Algorithm-Reject is $-8795. We also see that the loss histogram left-skewed for the Algorithm-Select, whereas it is centered farther form 0 for Algorithm-Reject. This suggests that the algorithm, on average, the Algorithm is doing a good job of not losing money via defaults.\nWe now consider repayments.\n\n\n\n\n\n\n\n\n\nSelection Repayment Prfoit\n\n\n\n\n\n\n\nRejection Repayment Profit\n\n\n\n\n\n\nLoan Repayments for Alorithm Selections and Rejections\n\n\n\nAgain, of those individuals who repaid their loans, the average profit for Algorithm-Select is $2713. However the average profit in the Algorithm-Reject group is 5780. We see that the histogram for the Algorithm-Select group is right skewed, whereas the histogram for Algorithm-Reject has a mound in $4000 - $6000 dollar range. The algorithm is not extending loans to potentially profitable individuals. Recall that one of the features we trained the model on was loan_percent income. While this was a good predictor of loan defaults, it (unsurprisingly) also has a positive correlation with loan amount for both default and repay groups:\n\nThis can make it difficult for our algorithm to correctly appraise high value loans leading to unrealized gains. The average profit for the training set was $1,630 per borrower, compared to $1,575 per borrower on the testing set. This small difference lends credence to the idea that our model is having trouble distinguishing profitable individuals from unprofitable ones. In that sense, this is a question of model selection rather than optimization. Given the overlapping nature of the features in the data set, and the fact that we are working with a linear model, it is unlikely that a better feature set exists. Considering our algorithm is profitable, I would say that it receives a satisfactory grade from the bank."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/index.html#evaluation---demographic-impact",
    "href": "posts/OptimalDecisionMaking/index.html#evaluation---demographic-impact",
    "title": "Optimal Decision Making",
    "section": "Evaluation - Demographic Impact",
    "text": "Evaluation - Demographic Impact\nWe’ll now investigate the impact the model has on potential borrowers. We’ll use the same features we discussed at the beginning. First, let’s look at loan intent breakdown.\n\nAcross all loan intent types, we see that selection percent is just north of 80. Although there is likely a statistically significant difference among intent groups, it is not dramatic enough to suggest a high degree of harm. This is good, since it means that individuals seeking loans for important things like education or medical expenses will not be discriminated against. We might also examine age:\n\nAgain, all age brackets have just north of 80% selection. The major exception is the 65+ group, which has a selection rate just under 60%. This suggests that our algorithm is biased against older individuals, likely because they have lower incomes and a higher likelihood to default on loans relative to the population at large. Finally, we might consider Home Ownership Type, as this was an input feature to our model.\n\n\n\n\n\n\n\n\n\nSelection Ownership Types\n\n\n\n\n\n\n\nRejection Ownership Types\n\n\n\n\n\n\nHomeownership Type for Algorithm Selections and Rejections\n\n\n\nWe see that renters compose of 45.8% of Individuals seleclted for a loan, but 84.9% of individuals whose loan application was rejected. Individuals with mortgages or who own homes make up about half of all loan selections, and around 14% of all loan rejections. If we make the same chart on the testing data, but instead look at historic defaults and repayments, we see that renters make up about 73% of individuals who default on their loans. Homeowners and individuals mortgages make up about a quarter of loan defaults. Clearly, the algorithm we designed has made it harder for renters to get loans, thereby exacerbating existing inequality.\nMoreover, looking at the loan percent income by selection type used in the analysis for the bank, we see that our algorithm tends to select Individuals where loans are a lower portion of income. Commodities like healthcare and education tend to have a fixed, absolute cost regardless of economic background. Hence, this can make it harder for less affluent individuals to achieve wellbeing and economic advancement. This is confirmed looking at the mean income which is $70074 and $42644 for select and rejection groups respectively. Perhaps even more paradoxically, if homeownership / mortgage is a critical component in receiving a loan, how can we expect renters to own homes if they cannot receive a loan to buy one in the first place?\nGiven this information, we might think that profitability is at odds with social responsibility (i.e. equality in loan extension). The synthesis there is notice that the bank left some profit on the table–some of which could be gained by lending to a more diverse group with respect to ownership type. Perhaps we need more features that partition the dataset, or a better model through which the data can be trained on. Or perhaps we should not be doing machine learning here at all. Maybe numbers will never represent abstract notions of trustworthiness and interpersonal relations. A question for another time (or blog post?)."
  },
  {
    "objectID": "posts/OptimalDecisionMaking/index.html#utility-functions",
    "href": "posts/OptimalDecisionMaking/index.html#utility-functions",
    "title": "Optimal Decision Making",
    "section": "Utility Functions",
    "text": "Utility Functions\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('future.no_silent_downcasting', True)\n\n\ndef loadData(train_url, fileName):\n    #loads training data as pickle object in local dir\n    csv = fileName + \".pkl\"\n    try:\n\n        df = pd.read_pickle(csv)\n\n    except FileNotFoundError:\n\n        print(\"Bootstrapping...\")\n        df = pd.read_csv(train_url)\n        df.to_pickle(csv)\n\n    return df\n\ndef clean(credit):\n    #converts categoricals to numerics and drops NA\n    ownershipMapping = {\"MORTGAGE\": 0, \"OWN\": 1, \"RENT\": 2, \"OTHER\": 3, \"Unknown\": -1}\n    defaultMapping = {\"N\": 0, \"Y\": 1, \"Unknown\": -1}\n    intentMapping = {\"VENTURE\": 0, \"EDUCATION\": 1, \"MEDICAL\": 2, \"HOMEIMPROVEMENT\": 3, \"PERSONAL\": 4, \"DEBTCONSOLIDATION\" : 5, \"Unknown\": -1}\n    gradeMapping = {'B':1,  'C': 2, 'A':0, 'D':3, 'E':4, 'F':4, 'G':5}\n    credit[\"person_home_ownership\"] = credit[\"person_home_ownership\"] = credit[\"person_home_ownership\"].replace(ownershipMapping).ffill()\n    credit[\"cb_person_default_on_file\"] = credit[\"cb_person_default_on_file\"] = credit[\"cb_person_default_on_file\"].replace(defaultMapping).ffill()\n    credit[\"loan_intent\"] = credit[\"loan_intent\"] = credit[\"loan_intent\"].replace(intentMapping).ffill()\n    credit[\"loan_grade\"] = credit[\"loan_grade\"] = credit[\"loan_grade\"].replace(gradeMapping).ffill()\n    credit = credit.dropna()\n    return credit\n\ndef sampleMaxProfit(credit, n, k, scoreRange):\n    #samples size n from df credit.\n    #tests tresholds across \n    T = scoreRange[0]\n    maxTuple = [0, 0]\n    sample = credit.sample(n)\n    delta_T = (abs(scoreRange[1] - scoreRange[0]))/k\n    while T &lt; scoreRange[1]: #can we do this without a loop?\n        PnL = sample.loc[sample['probability_score'] &lt;= T, 'PnL'].sum()\n        if (PnL &gt; maxTuple[0]):\n            maxTuple = [PnL, T]\n        T += delta_T\n    return maxTuple[1] \n\ndef optimalProfitScore(credit, iterations, n, k):\n    #Finds treshold score T that optmizes profit\n    testResult = []\n    scoreRange = [credit[\"probability_score\"].min(), credit[\"probability_score\"].max()]\n    for i in range(0, iterations): # can we do this without a loop??\n        testResult.append(sampleMaxProfit(credit, n, k, scoreRange))\n    npTestResult = np.array(testResult)\n    return 1.06 * np.mean(npTestResult)\n\ndef generateSampleGraph(df, n, k, t_star):\n    #initialize\n    scoreRange = [df[\"probability_score\"].min(), df[\"probability_score\"].max()]\n    t_values = []\n    PnL_values = []\n    T = scoreRange[0]\n    sample = df\n    delta_T = (abs(scoreRange[1] - scoreRange[0]))/k\n    tMax = [0, 0] #[T, P(T)]\n    #collect data\n    while T &lt; scoreRange[1]: #can we do this without a loop?\n        PnL = sample.loc[sample['probability_score'] &lt; T, 'PnL'].sum()\n        t_values.append(T)\n        PnL_values.append(PnL)\n        if PnL &gt; tMax[1]:\n            tMax = [T, PnL]\n        T += delta_T\n    print(\"Maximum T: \" + str(tMax[0]))\n    print(\"T*: \" + str(t_star))\n    #make graph\n    plt.scatter(t_values, PnL_values)\n    #plot red line\n    vertical_line_x = tMax[0]\n    if (t_star &gt; 0):\n        plt.axvline(x=t_star, color='orange', linestyle='--', alpha = 0.7)\n    # Plot the vertical line\n    plt.axvline(x=vertical_line_x, color='red', linestyle='--', alpha = 0.7)\n    # Add labels and title and make pretty\n    plt.xlabel('Threshold [T]', font = \"Courier\")\n    plt.ylabel('PnL [P(T)]', font = \"Courier\")\n    plt.title('Treshold vs PnL', font = \"Courier\")\n    plt.xticks(fontname='Courier', fontsize=10)  # Change font to Courier\n    plt.yticks(fontname='Courier', fontsize=10) \n    plt.show()"
  },
  {
    "objectID": "posts/OptimalDecisionMaking/index.html#data-visualization-and-model",
    "href": "posts/OptimalDecisionMaking/index.html#data-visualization-and-model",
    "title": "Optimal Decision Making",
    "section": "Data Visualization and Model",
    "text": "Data Visualization and Model\n\nimport utility\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nDELTA_T = 0.5\nURL_TRAIN = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\nURL_TEST = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\nTRAIN_FILE_NAME = \"creditTrainingData\"\nTEST_FILE_NAME = \"creditTestingData\"\n\ncredit = utility.loadData(URL_TRAIN, TRAIN_FILE_NAME)\ncreditTest = utility.loadData(URL_TEST, TEST_FILE_NAME)\ncredit = utility.clean(credit)\ncreditTest = utility.clean(creditTest)\n\n\n#DATA VISUALIATION OF METRICS OF INTERESTS\ninterestingFeatures = [\"loan_percent_income\", \"cb_person_default_on_file\", \"person_home_ownership\"]\n#interestingFeatures = [\"cb_person_default_on_file\", \"person_home_ownership\", \"loan_grade\"]\n\nsns.pairplot(credit.select_dtypes(include=['int', 'float']), hue = \"loan_status\", kind='reg', height = 1, aspect = 1)\nplt.legend().remove()\nplt.show()\n\ncorrelation_matrix = credit.select_dtypes(include=['int', 'float']).corr()\ncorrelation_matrix = credit.corr()\nplt.show()\n\n# #Plot the correlation matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", annot_kws={\"size\": 10})\nplt.title('Correlation Matrix')\nplt.show()\n\n#high percent income group by HOME_OWNERSHIP\nnum_bins = 6\nprint(\"here\")\nhist_data = []\nownershipMapping = {0: \"MORTGAGE\", 1: \"OWN\", 2: \"RENT\", 3: \"OTHER\", -1 : \"Unknown\"}\ncolors = [\"#82C272\", \"#00A88F\", \"#0087AC\", \"#005FAA\"]\ncreditTemp = credit[credit[\"loan_percent_income\"] &lt;= .5]\nunique_categories = creditTemp[\"person_home_ownership\"].unique()\n# Loop through each category value\nfor category_value in unique_categories:\n    # Filter DataFrame for the current category\n    category_data = creditTemp[creditTemp[\"person_home_ownership\"] == category_value]['loan_percent_income']\n    # Append the data to the list\n    hist_data.append(category_data)\nplt.hist(hist_data, bins=num_bins, stacked=True, label=unique_categories, color = colors)\n# Add labels, title, and make pretty\nplt.xlabel(\"Loan Percent of Income\", font=\"Courier\")\nplt.ylabel(\"Number of Borrowers by Home Ownership Type\", font=\"Courier\")\nplt.title(\"Loan Percent of Income by Homeownership\", font=\"Courier\")\nlegend_labels = [ownershipMapping[numericCode] for numericCode in unique_categories]\nplt.legend(labels=legend_labels)\nplt.xticks(fontname='Courier', fontsize=10)  # Change font to Courier\nplt.yticks(fontname='Courier', fontsize=10) \n# Show plot\nplt.show()\n\n\n\nstatusMapping = {0: \"Repaid\", 1: \"Defaulted\"}\n\n#Loan Default Status by Loan Amounts\nnum_bins = 6\nhist_data = []\ncolors = [\"#82C272\", \"#00A88F\"]#\"#0087AC\"]# \"#005FAA\"]\ncreditTemp = credit\nunique_categories = creditTemp[\"loan_status\"].unique()\n# Loop through each category value\nfor category_value in unique_categories:\n    # Filter DataFrame for the current category\n    category_data = creditTemp[creditTemp[\"loan_status\"] == category_value]['loan_amnt']\n    # Append the data to the list\n    hist_data.append(category_data)\nplt.hist(hist_data, bins=num_bins, stacked=True, label=unique_categories, color = colors)\n# Add labels, title, and make pretty\nplt.xlabel(\"Loan Amount ($)\", font=\"Courier\")\nplt.ylabel(\"Frequency\", font=\"Courier\")\nplt.title(\"Loan Defaults and Repayments by Amount\", font=\"Courier\")\nlegend_labels = [statusMapping[numericCode] for numericCode in unique_categories]\nplt.legend(labels=legend_labels)\nplt.xticks(fontname='Courier', fontsize=10)  # Change font to Courier\nplt.yticks(fontname='Courier', fontsize=10) \n# Show plot\nplt.show()\n\n#PIE #1\nstatusMapping = {0: \"Repaid\", 1: \"Defaulted\"}\n#Plotting Loan Status by Intent \ncolors = [\"green\", \"#82C272\", \"#00A88F\", \"#0087AC\", \"#005FAA\", \"#323B81\"]\nintentMapping = {0: \"Venture\", 1: \"Education\", 2: \"Medical\", 3: \"Home Improvement\", 4: \"Personal\", 5: \"Debt Consolidation\", -1: \"Unknown\"}\nloan_status_groups = credit[\"loan_status\"].unique()\nfor status in loan_status_groups:\n    group_data = credit[credit[\"loan_status\"] == status]\n    category_counts = group_data[\"loan_intent\"].value_counts().sort_index()\n    labels = [intentMapping.get(x) for x in category_counts.index]\n    sizes = category_counts.values\n\n    plt.figure()\n    pie = plt.pie(sizes, labels = labels, autopct=\"%1.1f%%\", startangle = 140, colors = colors)\n    plt.title(\"Intent Breakdown for \" + str(statusMapping[status]) + \" Loans\", fontname = \"Courier\", fontsize = 14)\n    #make pretty\n    for text in pie[1]:\n        text.set_fontname(\"Courier\")\nplt.show()\n\n#statistical test\ncontingency_table = pd.crosstab(credit[\"loan_status\"], credit[\"loan_intent\"])\nchi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency_table)\nprint(\"Chi-square statistic\", chi2_stat)\nprint(\"P-value\", p_val)\n\n#PIE #2\n\n#Plotting Loan Status by Ownership tyoe\ncolors = [\"green\", \"#82C272\", \"#00A88F\", \"#0087AC\", \"#005FAA\", \"#323B81\"]\nownershipMapping = {0: \"MORTGAGE\", 1: \"OWN\", 2: \"RENT\", 3: \"OTHER\", -1 : \"Unknown\"}\nloan_status_groups = credit[\"loan_status\"].unique()\nloan_status_groups = loan_status_groups\nfor status in loan_status_groups:\n    group_data = credit[credit[\"loan_status\"] == status]\n    category_counts = group_data[\"person_home_ownership\"].value_counts().sort_index()\n    labels = [ownershipMapping.get(x) for x in category_counts.index]\n    sizes = category_counts.values\n\n    plt.figure()\n    pie = plt.pie(sizes, labels = labels, autopct=\"%1.1f%%\", startangle = 140, colors = colors)\n    plt.title(\"Home Ownership Breakdown for \" + str(statusMapping[status]) + \" Loans\", font = \"Courier\")\n     #make pretty\n    for text in pie[1]:\n        text.set_fontname(\"Courier\")\n\nplt.show()\n\nstatusMapping = {0: \"Repaid\", 1: \"Defaulted\"}\n#statistical test\ncontingency_table = pd.crosstab(credit[\"loan_status\"], credit[\"person_home_ownership\"])\nchi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency_table)\nprint(\"Chi-square statistic\", chi2_stat)\nprint(\"P-value\", p_val)\n\n# #statistical test for HOME OWNERSHIP VS LOAN AMOUNT\n# x = credit[credit[\"loan_percent_income\"] &lt;= .5]\n# contingency_table = pd.crosstab(credit[credit[\"loan_percent_income\"] &gt; .5], credit[\"person_home_ownership\"])\n# chi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n# print(\"Chi-square statistic\", chi2_stat)\n# print(\"P-value\", p_val)\n\n\n#PLOTTING LOAN AMOUNT VS INCOME\n\n# custom_colors = {0: \"#82C272\", 1 :\"#0087AC\"}\n# sns.set(style = 'whitegrid', font = \"Courier\")\n# for key, value in custom_colors.items():\n#     custom_colors[key] = value + '80'\n# sns.scatterplot(x = \"person_income\", y = \"loan_amnt\", hue = \"loan_status\", palette = custom_colors, data = credit)\n# plt.xlim(-1000000, 1000000)\n# plt.title(\"Loan Amount vs. Income\")\n# plt.xlabel(\"Loan Amount\")\n# plt.ylabel(\"Person Income\")\n# plt.show()\n\nprint(\"fitting model\")\n\nLR = LogisticRegression()\nLR.fit(credit[interestingFeatures], credit[\"loan_status\"])\n#scoring model\npred = LR.predict(creditTest[interestingFeatures])\ncredit[\"probability_score\"] = LR.predict_proba(credit[interestingFeatures])[:, 1]\ncreditTest[\"probability_score\"] = LR.predict_proba(creditTest[interestingFeatures])[:, 1]\n\naccuracy = accuracy_score(pred, creditTest[\"loan_status\"])\n\n#creating score columns\n# credit[\"score\"] =  weights[0][0] * credit[\"loan_percent_income\"] + weights[0][1] * credit[\"cb_person_default_on_file\"] + weights[0][2] * credit[\"person_home_ownership\"]\n# creditTest[\"score\"] = weights[0][0] * creditTest[\"loan_percent_income\"] + weights[0][1] * creditTest[\"cb_person_default_on_file\"] + weights[0][2] * creditTest[\"person_home_ownership\"]\n\n#creating PnL columns\npercentProfit = .25\ncredit.loc[credit[\"loan_status\"] == 0, \"PnL\"] = credit[\"loan_amnt\"] * (1 +  percentProfit * .01 * credit[\"loan_int_rate\"])**10 - credit[\"loan_amnt\"]\ncredit.loc[credit[\"loan_status\"] == 1, \"PnL\"] = credit[\"loan_amnt\"]* (1 +  percentProfit * .01 *credit[\"loan_int_rate\"])**3 - 1.7 * credit[\"loan_amnt\"]\ncreditTest[\"loan_status_pred\"] = pred\ncreditTest.loc[creditTest[\"loan_status\"] == 0, \"PnL\"] = creditTest[\"loan_amnt\"] * (1 +  percentProfit * .01 * creditTest[\"loan_int_rate\"])**10 - creditTest[\"loan_amnt\"]\ncreditTest.loc[creditTest[\"loan_status\"] == 1, \"PnL\"] = creditTest[\"loan_amnt\"]* (1 +  percentProfit * .01 * creditTest[\"loan_int_rate\"])**3 - 1.7 * creditTest[\"loan_amnt\"]\n\n# print(credit.loc[credit[\"loan_status\"] == 0, \"probability_score\"].mean())\n# print(credit.loc[credit[\"loan_status\"] == 1, \"probability_score\"].mean())\n\nBankPnL = creditTest.loc[creditTest['loan_status'] == 0, 'PnL'].mean()\nmachinePnL = creditTest.loc[creditTest[\"loan_status_pred\"] == 0, 'PnL'].mean()\nT = utility.optimalProfitScore(credit, 200, 100, 50)\ntresholdPnL = creditTest.loc[creditTest[\"probability_score\"] &lt; T, \"PnL\"].mean()\ntheoreticalPnL = creditTest.loc[creditTest[\"PnL\"] &gt; 0, \"PnL\"].mean()\n\ncreditTest[\"ALGO_DECISION\"] = creditTest[\"probability_score\"] &lt; T\n\n#BANK INVESTIGATION\n#analyzing loan default\n#means\nmeanPopDeafultValue = creditTest[creditTest[\"loan_status\"] == 1][\"PnL\"].mean()\nmeanAlgoSelectDefaultValue = creditTest[(creditTest[\"ALGO_DECISION\"] == True) & (creditTest[\"loan_status\"] == 1)][\"PnL\"].mean()\nmeanAlgoRejectDefaultValue = creditTest[(creditTest[\"ALGO_DECISION\"] == False) &  (creditTest[\"loan_status\"] == 1)][\"PnL\"].mean()\n#histograms\n#ALGO accept\nplt.hist(creditTest[(creditTest[\"ALGO_DECISION\"] == True) & (creditTest[\"loan_status\"] == 1)][\"PnL\"], bins=6, color='green', edgecolor='black')\nplt.xlabel('Loan Profit', font = \"Courier\")\nplt.ylabel('Frequency', font = \"Courier\")\nplt.title('Loan Profitability Distriubtion for Algorithm Selections', font = \"Courier\")\nplt.xticks(fontname='Courier', fontsize=10)  # Change font to Courier\nplt.yticks(fontname='Courier', fontsize=10) \nplt.grid(True)\nplt.show()\n#ALGO reject\nplt.hist(creditTest[(creditTest[\"ALGO_DECISION\"] == False) & (creditTest[\"loan_status\"] == 1)][\"PnL\"], bins=6, color='green', edgecolor='black')\nplt.xlabel('Loan Profit', font = \"Courier\")\nplt.ylabel('Frequency', font = \"Courier\")\nplt.title('Loan Profitability Distriubtion for Algorithm Rejections', font = \"Courier\")\nplt.xticks(fontname='Courier', fontsize=10)  # Change font to Courier\nplt.yticks(fontname='Courier', fontsize=10) \nplt.grid(True)\nplt.show()\n\n#print outs\nprint(\"DEFAULT\")\nprint(\"Population Mean: \" + str(meanPopDeafultValue))\nprint(\"Algorithm SELECT Mean: \" + str(meanAlgoSelectDefaultValue))\nprint(\"Algorithm REJECT Mean \" + str(meanAlgoRejectDefaultValue))\n\n#alayzing loan repayment\nmeanPopRepayValue =  creditTest[creditTest[\"loan_status\"] == 0][\"PnL\"].sum()\nmeanAlgoSelectRepayValue = creditTest[(creditTest[\"ALGO_DECISION\"] == True) & (creditTest[\"loan_status\"] == 0)][\"PnL\"].mean()\nmeanAlgoRejectRepayValue = creditTest[(creditTest[\"ALGO_DECISION\"] == False) & (creditTest[\"loan_status\"] == 0)][\"PnL\"].mean()\n#print outs\nprint(\"REPAID\")\nprint(\"Population Mean: \" + str(meanPopRepayValue))\nprint(\"Algorithm SELECT Mean: \" + str(meanAlgoSelectRepayValue))\nprint(\"Algorithm REJECT Mean \" + str(meanAlgoRejectRepayValue))\n#histograms\n#ALGO accept\nplt.hist(creditTest[(creditTest[\"ALGO_DECISION\"] == True) & (creditTest[\"loan_status\"] == 0)][\"PnL\"], bins=6, color='green', edgecolor='black')\nplt.xlabel('Loan Profit', font = \"Courier\")\nplt.ylabel('Frequency', font = \"Courier\")\nplt.title('Loan Profitability Distriubtion for Algorithm Selections', font = \"Courier\")\nplt.xticks(fontname='Courier', fontsize=10)  # Change font to Courier\nplt.yticks(fontname='Courier', fontsize=10) \nplt.grid(True)\nplt.show()\n#ALGO reject\nplt.hist(creditTest[(creditTest[\"ALGO_DECISION\"] == False) & (creditTest[\"loan_status\"] == 0)][\"PnL\"], bins=6, color='green', edgecolor='black')\nplt.xlabel('Loan Profit', font = \"Courier\")\nplt.ylabel('Frequency', font = \"Courier\")\nplt.title('Loan Profitability Distriubtion for Algorithm Rejections', font = \"Courier\")\nplt.xticks(fontname='Courier', fontsize=10)  # Change font to Courier\nplt.yticks(fontname='Courier', fontsize=10) \nplt.grid(True)\nplt.show()\n\n#Printing Loan Percent of Income by Algorithm Decision Type\nstatusMapping = {True:\"Algorithm Select\", False:\"Algorithm Reject\"}\n\nfor algo_status in creditTest[\"ALGO_DECISION\"].unique():\n\n    plt.hist(creditTest[creditTest[\"ALGO_DECISION\"] == algo_status][\"loan_percent_income\"], bins=6, color='skyblue', edgecolor='black')\n    plt.xlabel('Loan Percent of Income', font = \"Courier\")\n    plt.ylabel('Frequency', font = \"Courier\")\n    plt.title(\"Loan Percent Income for \" + statusMapping[algo_status], font = \"Courier\")\n    plt.xticks(fontname='Courier', fontsize=10)  # Change font to Courier\n    plt.yticks(fontname='Courier', fontsize=10) \n    plt.grid(True)\n    plt.show()\n\n\ncustom_colors = {0: \"#82C272\", 1 :\"#0087AC\"}\nsns.set(style = 'whitegrid', font = \"Courier\")\nfor key, value in custom_colors.items():\n    custom_colors[key] = value + '80'\nsns.scatterplot(x = \"loan_amnt\", y = \"loan_percent_income\", hue = \"loan_status\", palette = custom_colors, data = creditTest)\nplt.title(\"Loan Amount vs. Loan Percent Income\")\nplt.xlabel(\"Loan Amount\")\nplt.ylabel(\"Percent of Borrower Income\")\nplt.show()\n\n\n\n#generates profit curves and prints out useful metrics\nutility.generateSampleGraph(credit, 100, 500, T)\nutility.generateSampleGraph(creditTest, 100, 500, T)\n\nprint(\"Model Accuracy: %\" + str(accuracy))\nprint(\"Bank PnL: $\" + str(BankPnL))\n#print(\"Model PnL: $\" + str(machinePnL))\nprint(\"Tresholded PnL: $\" + str(tresholdPnL))\nprint(\"Loss: \" + str(tresholdPnL/BankPnL))\n\n#DEMOGRAPHIC INVESTIGATION\nstatusMapping = {True:\"Algorithm Select\", False:\"Algorithm Reject\"}\n\n#Plotting Loan Status by Intent \ncolors = [\"green\", \"#82C272\", \"#00A88F\", \"#0087AC\", \"#005FAA\", \"#323B81\"]\nintentMapping = {0: \"Venture\", 1: \"Education\", 2: \"Medical\", 3: \"Home Improvement\", 4: \"Personal\", 5: \"Debt Consolidation\", -1: \"Unknown\"}\nloan_status_groups = creditTest[\"ALGO_DECISION\"].unique()\nprint(loan_status_groups)\nfor status in loan_status_groups:\n    if status &lt; 0:\n        break\n    group_data = creditTest[creditTest[\"ALGO_DECISION\"] == status]\n    category_counts = group_data[\"loan_intent\"].value_counts()\n    labels = [intentMapping.get(x) for x in category_counts.index]\n    sizes = category_counts.values\n\n    plt.figure()\n    pie = plt.pie(sizes, labels = labels, autopct=\"%1.1f%%\", startangle = 140, colors = colors)\n    plt.title(\"Intent Breakdown for \" + str(statusMapping[status]) + \" Loans\", fontname = \"Courier\", fontsize = 14)\n    #make pretty\n    for text in pie[1]:\n        text.set_fontname(\"Courier\")\n   \nplt.show()\n\n#PIE #3\n\n#Plotting Loan Status by Ownership tyoe\ncolors = [ \"#82C272\", \"#00A88F\", \"#0087AC\", \"#005FAA\", \"#323B81\"]\nownershipMapping = {0: \"MORTGAGE\", 1: \"OWN\", 2: \"RENT\", 3: \"OTHER\", -1 : \"Unknown\"}\nloan_status_groups = creditTest[\"ALGO_DECISION\"].unique()\nfor status in loan_status_groups:\n    if status &lt; 0:\n        break\n    group_data = creditTest[creditTest[\"ALGO_DECISION\"] == status]\n    category_counts = group_data[\"person_home_ownership\"].value_counts().sort_index()\n    labels = [ownershipMapping.get(x) for x in category_counts.index]\n    sizes = category_counts.values\n\n    plt.figure()\n    pie = plt.pie(sizes, labels = labels, autopct=\"%1.1f%%\", startangle = 140, colors = colors)\n    plt.title(\"Home Ownership Breakdown for \" + str(statusMapping[status]) + \" Loans\", font = \"Courier\")\n     #make pretty\n    for text in pie[1]:\n        text.set_fontname(\"Courier\")\n\nplt.show()\n\n#PIE #4\n\n#Plotting Loan Status by Ownership tyoe\nstatusMapping = {0: \"Repaid\", 1: \"Defaulted\"}\ncolors = [\"green\", \"#82C272\", \"#00A88F\", \"#0087AC\", \"#005FAA\", \"#323B81\"]\nownershipMapping = {0: \"MORTGAGE\", 1: \"OWN\", 2: \"RENT\", 3: \"OTHER\", -1 : \"Unknown\"}\nloan_status_groups = creditTest[\"loan_status\"].unique()\nfor status in loan_status_groups:\n    group_data = creditTest[creditTest[\"loan_status\"] == status]\n    category_counts = group_data[\"person_home_ownership\"].value_counts().sort_index()\n    labels = [ownershipMapping.get(x) for x in category_counts.index]\n    sizes = category_counts.values\n\n    plt.figure()\n    pie = plt.pie(sizes, labels = labels, autopct=\"%1.1f%%\", startangle = 140, colors = colors)\n    plt.title(\"Home Ownership Breakdown for \" + str(statusMapping[status]) + \" Loans\", font = \"Courier\")\n     #make pretty\n    for text in pie[1]:\n        text.set_fontname(\"Courier\")\n\nplt.show()\n\n#Plotting Age\nage_bins = [18, 25, 35, 45, 55, 65, float(\"inf\")]\nage_labels = [\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"]\ncreditTest[\"ALGO_DECISION\"] = creditTest[\"ALGO_DECISION\"].astype(int)\ncreditTest[\"person_age\"] = pd.cut(creditTest[\"person_age\"], bins = age_bins, labels = age_labels, right= False)\npercentage_by_age_group = creditTest.groupby(\"person_age\")[\"ALGO_DECISION\"].mean() * 100\n\npercentage_by_age_group.plot(kind = \"bar\", color = \"blue\")\nplt.title(\"Loan Selection Percent by Age Group\")\nplt.xlabel(\"Age Group\")\nplt.ylabel(\"Selection Percent\")\nplt.xticks(rotation = 45)\nplt.ylim(0, 100)\nplt.tight_layout()\nplt.show()\n\n#Plotting by Intent\nintentMapping = {0: \"Venture\", 1: \"Education\", 2: \"Medical\", 3: \"Home Improvement\", 4: \"Personal\", 5: \"Debt Consolidation\", -1: \"Unknown\"}\ncreditTest[\"loan_intent\"] = creditTest[\"loan_intent\"].map(intentMapping)\ncreditTest[\"ALGO_DECISION\"] = creditTest[\"ALGO_DECISION\"].astype(int)\npercentage_by_age_group = creditTest.groupby(\"loan_intent\")[\"ALGO_DECISION\"].mean() * 100\npercentage_by_age_group.plot(kind = \"bar\", color = \"blue\")\nplt.title(\"Selection Percent by Loan Intent\")\nplt.xlabel(\"Loan Intent\")\nplt.ylabel(\"Selection Percent\")\nplt.xticks(rotation = 45)\nplt.ylim(0, 100)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Logistic Regression\n\n\n\n\n\nImplementing Logistic Regression with Momentum in PyTorch.\n\n\n\n\n\nApr 14, 2024\n\n\nAlec Kyritsis\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\nImplementing Logistic Regression with Momentum in PyTorch.\n\n\n\n\n\nApr 14, 2024\n\n\nAlec Kyritsis\n\n\n\n\n\n\n\n\n\n\n\n\nWomen in Data Science\n\n\n\n\n\nAnalyzing the Leaky STEM Pipeline\n\n\n\n\n\nMar 28, 2024\n\n\nAlec Kyritsis\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins!\n\n\n\n\n\nClassifying Penguins\n\n\n\n\n\nApr 2, 2023\n\n\nAlec Kyritsis\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Decision Making\n\n\n\n\n\nMaximzing bank profit on loan returns using logistic regression and a sampling-determined threshold.\n\n\n\n\n\nMar 31, 2023\n\n\nAlec Kyritsis\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]