<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alec Kyritsis">
<meta name="dcterms.date" content="2024-04-14">
<meta name="description" content="Implementing Logistic Regression with Momentum in PyTorch.">

<title>My Awesome CSCI 0451 Blog - Logistic Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: white;
      }

      .quarto-title-block .quarto-title-banner {
        color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
      }
</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Awesome CSCI 0451 Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Logistic Regression</h1>
                  <div>
        <div class="description">
          Implementing Logistic Regression with Momentum in PyTorch.
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alec Kyritsis </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 14, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div id="cell-1" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Packages and Style</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> logistic <span class="im">import</span> LogisticRegression</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> logistic <span class="im">import</span> Utility</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'font.family'</span>] <span class="op">=</span> <span class="st">"Courier"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>We explore the mathematics that inform regression models and implement Logistic Regression in PyTorch. We perform tests to assess the correctness, speed, and generializability of the model. The code may be found at https://github.com/a1kyritsis/a1kyritsis_blog.github.io/tree/main/posts/logisticRegression.</p>
</section>
<section id="logistic-regression" class="level1">
<h1>Logistic Regression</h1>
<p>Logistic Regression is a popular, intuitive form of a machine learning. At a high level, given some set of features, we hope to weight said features in such a way that we minimize the number of mispredictions. To do so, we transform this to a multivariate minimization problem, where we find the global minimum of a convex function. We provide a brief, semi-formal overview:</p>
<p>Suppose we have some data matrix <span class="math inline">\(\mathbb{X}\)</span> with <span class="math inline">\(n\)</span> observations and set of features <span class="math inline">\(F\)</span>. Also suppose we believe <span class="math inline">\(P \subseteq F, \; |P| = p\)</span> to be a set of features with some deal of predictive power. Let <span class="math inline">\(X \subseteq \mathbb{X}\)</span> be a <span class="math inline">\(n \times p\)</span> matrix formed by indexing columns of <span class="math inline">\(\mathbb{X}\)</span> with <span class="math inline">\(P\)</span>. We distinguish between data matrix <span class="math inline">\(\mathbb{X}\)</span> and <span class="math inline">\(X\)</span> since we usually do not train on all features. Let <span class="math inline">\(l\)</span> denote an arbitrary loss function, <span class="math inline">\(\vec{w}\)</span> the set of feature weights, <span class="math inline">\(\vec{x_i}\)</span> the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(X\)</span>, and <span class="math inline">\(y_i \in \{0, 1\}\)</span> the binary label of <span class="math inline">\(x_i\)</span>. We then hope to solve for the optimal set of weights <span class="math inline">\(\hat{w}\)</span> such that</p>
<p><span class="math display">\[\hat{w} = \text{argmin}_{\vec{w}} \; \left[ \frac{1}{n} \sum_{i = 1}^n l(&lt;\vec{w}, \vec{x_i}&gt;, y_i) \right].\]</span></p>
<p>In other words, the set of weights <span class="math inline">\(\hat{w}\)</span> that minmizes loss across all observations. One problem is that our loss function may not have a global minimum which can be found. The dirty little trick of Logistic Regression is to choose a convex loss function when evaluating the per observation loss. This ensures that our global loss function is also convex. Since we are developing an algorithm for logistic regression, it makes sense that we choose the logistic loss function</p>
<p><span class="math display">\[l(s, y) = -y log\left[ \sigma(s) \right] - (1 - y)log\left[ 1 - \sigma(s) \right]\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the binary label and <span class="math inline">\(s\)</span> the score with respect to the <span class="math inline">\(i^{th}\)</span> observation. Here, <span class="math inline">\(\sigma(s)\)</span> is just the sigmoid function</p>
<p><span class="math display">\[\sigma(s) = \frac{1}{1 + exp\left[ - s \right]}.\]</span></p>
<p>Now that we have established our loss function, we may turn to the notion of gradient descent. In Calc I, we learned that the derivative of a function is its rate of change. This idea can be extended to multivariant functions in <span class="math inline">\(\mathbb{R}^p\)</span>. Rather than looking at the rate of change for a single variable, we look at the rate of change for each input <span class="math inline">\(x_i, \; i \le p\)</span>, holding all else constant. For some input tuple $ T = (x_1, …, x_p)$ and multivariant function <span class="math inline">\(G: \mathbb{R}^p \longrightarrow \mathbb{R}^k, \; k \in \mathbb{Z}^+\)</span>, we may evualuate <span class="math inline">\(T\)</span> at the gradient <span class="math inline">\(\nabla G(T)\)</span> to find the rate of change in each dimension.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="media/multi.jpg" class="img-fluid figure-img"></p>
<figcaption>Multivariable Convex Function in <span class="math inline">\(\mathbb{R}^3\)</span></figcaption>
</figure>
</div>
<p>Even better, remember in Calc I when we needed to find the minimum or maximum of a function, we set the derivative equal to <span class="math inline">\(0\)</span> and solved for the critical point. We can complete an identical (albeit slightly harder) process with the gradient and a multivariant function. Since <span class="math inline">\(L\)</span> is a convex function, we can then find its minimum using the gradient <span class="math inline">\(\nabla L\)</span>. We state the gradient of the Logistic Loss Function without proof:</p>
<p><span class="math display">\[ \nabla L(\vec{w}) =  \frac{1}{n} \sum_{i = 1}^n (\sigma(&lt;\vec{w}, \vec{x_i}&gt;) - y_i)x_i.\]</span></p>
<p>If we find where <span class="math inline">\(\nabla L(\vec{w}) = 0\)</span>, we find the set of weights <span class="math inline">\(\vec{w}\)</span> that minimizes loss and hence maximizes our chance of attaining a successful prediction.</p>
<p>The final hurdle we must then navigate is how to actual do this. Since it’s 2024, we could force the computer to do some algebra. However, this uses uncessary computational power and the algebra may not always be nice. One way to think of the gradient is as a vector tangent to a point on our function. We then use the gradient to “point” us in the direction of the minimum.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="media/grad.jpg" class="img-fluid figure-img"></p>
<figcaption>Gradient at Point of Function in <span class="math inline">\(\mathbb{R}^3\)</span></figcaption>
</figure>
</div>
<p>Since <span class="math inline">\(L\)</span> is convex (<span class="math inline">\(\mathcal{H}(L) &lt; 0\)</span>), its gradient is always negative, so we will always be descending. The process is then as follows: Pick some point on <span class="math inline">\(L\)</span>, call it <span class="math inline">\(w_1\)</span>. Evaluate <span class="math inline">\(\nabla L (w_1)\)</span>, to find <span class="math inline">\(w_2\)</span>. Repeat this process until some arbitrary stopping parameter is reached. Formally, at the <span class="math inline">\(k^{th}\)</span> iteration of our algorithm, we find <span class="math inline">\(w_{k + 1}\)</span> as follows:</p>
<p><span class="math display">\[ w_{k + 1} \longleftarrow w_k - \alpha \nabla L(w_k) + \beta (w_k - w_{k - 1}).\]</span></p>
<p>Here, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are two learning rate parameters which control the step size of our descent. The first parameter <span class="math inline">\(\alpha\)</span> controls the impact of the gradient. The second parameter <span class="math inline">\(\beta\)</span> controls the momentum term.</p>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>We walk through some of the important code chunks in our Logistic Regression algorithm. First, we score by taking the row-wise dot product, equivalent to matrix multiplication. If no weight vector is given, we initialize it:</p>
<div id="cell-5" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> score(<span class="va">self</span>, X):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.w <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.w <span class="op">=</span> torch.rand((X.size()[<span class="dv">1</span>]))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.matmul(X, <span class="va">self</span>.w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we define a prediction function. If the score of observation <span class="math inline">\(s_i\)</span> satisfies <span class="math inline">\(s_i &gt; 0\)</span>, then we have found a seperating hyperplane and correctly classified the data. We achieve this as follows:</p>
<div id="cell-7" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>        score_vector <span class="op">=</span> <span class="va">self</span>.score(X)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (score_vector <span class="op">&gt;</span> <span class="dv">0</span>).<span class="bu">int</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We move to larger code blocks. We calculate the gradient using the mentioned formula</p>
<div id="cell-9" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad(<span class="va">self</span>, X, y, s):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>        <span class="co">#computes the gradient</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="dv">1</span><span class="op">/</span>n) <span class="op">*</span> torch.<span class="bu">sum</span>((X <span class="op">*</span> (<span class="va">self</span>.sigmoid(s) <span class="op">-</span> y).unsqueeze(<span class="dv">1</span>)), dim <span class="op">=</span> <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then implement a gradient descent optimizer, which performs the update <span class="math inline">\(w_{k + 1} \longleftarrow w_k - \alpha \nabla L(w_k) + \beta (w_k - w_{k - 1})\)</span>:</p>
<div id="cell-11" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a> <span class="kw">def</span> gradientDescentOptimizer(<span class="va">self</span>, X, y, alpha, beta):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>        <span class="co">#spicy gradient descent</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.w_prev <span class="op">==</span> <span class="va">None</span>:</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.w_prev <span class="op">=</span> torch.zeros(X.shape[<span class="dv">1</span>]) <span class="co"># w_{k - 1}</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.w_next <span class="op">==</span> <span class="va">None</span>:</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.w_next <span class="op">=</span> torch.zeros(X.shape[<span class="dv">1</span>]) <span class="co">#w_{k + 1}</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> <span class="va">self</span>.score(X) <span class="co">#initialize score for w</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_next <span class="op">=</span> <span class="va">self</span>.w <span class="op">-</span> alpha <span class="op">*</span> <span class="va">self</span>.grad(X, y, s) <span class="op">+</span> beta <span class="op">*</span> (<span class="va">self</span>.w <span class="op">-</span> <span class="va">self</span>.w_prev) <span class="co">#update</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w_prev <span class="op">=</span> <span class="va">self</span>.w <span class="co">#reassign</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> <span class="va">self</span>.w_next</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, <span class="math inline">\(\texttt{self.W} = w_k\)</span>, <span class="math inline">\(\texttt{w\_prev} = w_{k - 1}\)</span>, and <span class="math inline">\(\texttt{w\_prev} = w_{k + 1}\)</span>. Finall for convenience, we also implement a training method that performs the looping functionality; or, the descent itself. The method records the number of iterations to converge and loss at each iteration.</p>
<div id="cell-13" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_logistic(X, y, alpha, beta, epsilon, LR):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        L <span class="op">=</span> LR.loss(X, y)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> []</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        step <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span>(torch.norm(LR.grad(X, y, LR.score(X))) <span class="op">&gt;</span> epsilon):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>            LR.gradientDescentOptimizer(X, y, alpha, beta)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>            L <span class="op">=</span> LR.loss(X, y)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            loss.append(L)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>            step <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [step, loss]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The condition in the <span class="math inline">\(\texttt{while}\)</span> loop is the stopping condition. Recall that our loss is minimized when <span class="math inline">\(\nabla L = 0\)</span>. In practice, this can be difficult to achieve, so we cheat by letting the gradient get close to <span class="math inline">\(0\)</span>, say some small positive constant <span class="math inline">\(\epsilon\)</span>. This enables us to tune both model accuracy and speed of convergence.</p>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<section id="vanilla-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="vanilla-gradient-descent">Vanilla Gradient Descent</h3>
<p>We first test a normal instance of gradient descent. Here, there is no momentum term so we take <span class="math inline">\(\beta = 0\)</span>. We set <span class="math inline">\(\alpha = .5\)</span>. For easy visualization, we set the number of features <span class="math inline">\(p = 2\)</span>, and the number of observations <span class="math inline">\(n = 100\)</span>. We also want to test if our algorithm is working on some easy data, so we will set the noise of the points <span class="math inline">\(\gamma = .12\)</span>. Finally, we set the <span class="math inline">\(\epsilon\)</span>-tolerence to <span class="math inline">\(.01\)</span>.</p>
<div id="cell-17" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Define parameters, testing data, and initialize model</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">.12</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> Utility.classification_data(n, gamma, p)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LogisticRegression()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>LR.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">.25</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.01</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">#Train the model and extract the weights</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> Utility.train_logistic(X, y, alpha, beta, epsilon, LR)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> LR.w</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co">#plotting decision boundary</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:,<span class="dv">0</span>], X[:, <span class="dv">1</span>], c <span class="op">=</span> y, cmap <span class="op">=</span> <span class="st">"coolwarm"</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>Utility.draw_line(w, <span class="op">-</span><span class="dv">2</span>, <span class="fl">2.5</span>, plt, color <span class="op">=</span> <span class="st">"red"</span>, linestyle <span class="op">=</span> <span class="st">"dashed"</span>, label <span class="op">=</span> <span class="st">"Decision Boundary"</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Feature 0"</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Feature 1"</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Test Points"</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co">#plotting loss over iterations</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>[num_iters, loss] <span class="op">=</span> res</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loss)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> torch.arange(<span class="dv">0</span>, num_iters)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loss) </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>plt.plot(i, loss, color <span class="op">=</span> <span class="st">"red"</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model Loss Over Iterations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[tensor(0.6644), tensor(0.6233), tensor(0.5773), tensor(0.5348), tensor(0.4970), tensor(0.4638), tensor(0.4344), tensor(0.4082), tensor(0.3847), tensor(0.3636), tensor(0.3445), tensor(0.3271), tensor(0.3112), tensor(0.2967), tensor(0.2833), tensor(0.2710), tensor(0.2597), tensor(0.2492), tensor(0.2395), tensor(0.2304), tensor(0.2220), tensor(0.2142), tensor(0.2068), tensor(0.1999), tensor(0.1935), tensor(0.1874), tensor(0.1817), tensor(0.1763), tensor(0.1712), tensor(0.1664), tensor(0.1619), tensor(0.1576), tensor(0.1535), tensor(0.1496), tensor(0.1459), tensor(0.1424), tensor(0.1390), tensor(0.1358), tensor(0.1327), tensor(0.1298), tensor(0.1270), tensor(0.1243), tensor(0.1217), tensor(0.1192), tensor(0.1169), tensor(0.1146), tensor(0.1124), tensor(0.1103), tensor(0.1083), tensor(0.1063), tensor(0.1044), tensor(0.1026), tensor(0.1008), tensor(0.0991), tensor(0.0975), tensor(0.0959), tensor(0.0944), tensor(0.0929), tensor(0.0914), tensor(0.0900), tensor(0.0887), tensor(0.0874), tensor(0.0861), tensor(0.0849), tensor(0.0837), tensor(0.0825), tensor(0.0814), tensor(0.0803), tensor(0.0792), tensor(0.0781), tensor(0.0771), tensor(0.0761), tensor(0.0752), tensor(0.0742), tensor(0.0733), tensor(0.0724), tensor(0.0715), tensor(0.0707), tensor(0.0699), tensor(0.0691), tensor(0.0683), tensor(0.0675), tensor(0.0667), tensor(0.0660), tensor(0.0653), tensor(0.0646), tensor(0.0639), tensor(0.0632), tensor(0.0626), tensor(0.0619), tensor(0.0613), tensor(0.0607), tensor(0.0600), tensor(0.0595), tensor(0.0589), tensor(0.0583), tensor(0.0577), tensor(0.0572), tensor(0.0567), tensor(0.0561), tensor(0.0556), tensor(0.0551), tensor(0.0546), tensor(0.0541), tensor(0.0536), tensor(0.0532), tensor(0.0527), tensor(0.0522), tensor(0.0518), tensor(0.0514), tensor(0.0509), tensor(0.0505), tensor(0.0501), tensor(0.0497), tensor(0.0493), tensor(0.0489), tensor(0.0485), tensor(0.0481), tensor(0.0477), tensor(0.0474), tensor(0.0470), tensor(0.0466), tensor(0.0463), tensor(0.0459), tensor(0.0456), tensor(0.0453), tensor(0.0449), tensor(0.0446), tensor(0.0443), tensor(0.0440), tensor(0.0437), tensor(0.0433), tensor(0.0430), tensor(0.0427), tensor(0.0424), tensor(0.0422), tensor(0.0419), tensor(0.0416), tensor(0.0413), tensor(0.0410), tensor(0.0408), tensor(0.0405), tensor(0.0402), tensor(0.0400), tensor(0.0397), tensor(0.0395), tensor(0.0392), tensor(0.0390), tensor(0.0387), tensor(0.0385), tensor(0.0382), tensor(0.0380), tensor(0.0378), tensor(0.0376), tensor(0.0373), tensor(0.0371), tensor(0.0369), tensor(0.0367), tensor(0.0365), tensor(0.0362), tensor(0.0360), tensor(0.0358), tensor(0.0356), tensor(0.0354), tensor(0.0352), tensor(0.0350), tensor(0.0348), tensor(0.0346), tensor(0.0344), tensor(0.0343), tensor(0.0341), tensor(0.0339), tensor(0.0337), tensor(0.0335), tensor(0.0333), tensor(0.0332), tensor(0.0330), tensor(0.0328), tensor(0.0327), tensor(0.0325), tensor(0.0323), tensor(0.0322), tensor(0.0320), tensor(0.0318), tensor(0.0317), tensor(0.0315), tensor(0.0314), tensor(0.0312), tensor(0.0310), tensor(0.0309), tensor(0.0307), tensor(0.0306), tensor(0.0305), tensor(0.0303), tensor(0.0302), tensor(0.0300), tensor(0.0299), tensor(0.0297), tensor(0.0296), tensor(0.0295), tensor(0.0293), tensor(0.0292), tensor(0.0291), tensor(0.0289), tensor(0.0288), tensor(0.0287), tensor(0.0285), tensor(0.0284), tensor(0.0283), tensor(0.0282), tensor(0.0280), tensor(0.0279), tensor(0.0278), tensor(0.0277), tensor(0.0276), tensor(0.0274), tensor(0.0273), tensor(0.0272), tensor(0.0271), tensor(0.0270), tensor(0.0269), tensor(0.0268), tensor(0.0266), tensor(0.0265), tensor(0.0264), tensor(0.0263), tensor(0.0262), tensor(0.0261), tensor(0.0260), tensor(0.0259), tensor(0.0258), tensor(0.0257), tensor(0.0256), tensor(0.0255), tensor(0.0254), tensor(0.0253), tensor(0.0252), tensor(0.0251), tensor(0.0250), tensor(0.0249), tensor(0.0248), tensor(0.0247), tensor(0.0246), tensor(0.0245), tensor(0.0244), tensor(0.0243), tensor(0.0242), tensor(0.0242), tensor(0.0241), tensor(0.0240), tensor(0.0239), tensor(0.0238), tensor(0.0237), tensor(0.0236), tensor(0.0235), tensor(0.0235), tensor(0.0234), tensor(0.0233), tensor(0.0232), tensor(0.0231), tensor(0.0230), tensor(0.0230), tensor(0.0229), tensor(0.0228), tensor(0.0227), tensor(0.0226), tensor(0.0226), tensor(0.0225), tensor(0.0224), tensor(0.0223), tensor(0.0223), tensor(0.0222), tensor(0.0221), tensor(0.0220), tensor(0.0220), tensor(0.0219), tensor(0.0218), tensor(0.0217), tensor(0.0217), tensor(0.0216), tensor(0.0215), tensor(0.0215), tensor(0.0214), tensor(0.0213), tensor(0.0212), tensor(0.0212), tensor(0.0211), tensor(0.0210), tensor(0.0210)]
[tensor(0.6644), tensor(0.6233), tensor(0.5773), tensor(0.5348), tensor(0.4970), tensor(0.4638), tensor(0.4344), tensor(0.4082), tensor(0.3847), tensor(0.3636), tensor(0.3445), tensor(0.3271), tensor(0.3112), tensor(0.2967), tensor(0.2833), tensor(0.2710), tensor(0.2597), tensor(0.2492), tensor(0.2395), tensor(0.2304), tensor(0.2220), tensor(0.2142), tensor(0.2068), tensor(0.1999), tensor(0.1935), tensor(0.1874), tensor(0.1817), tensor(0.1763), tensor(0.1712), tensor(0.1664), tensor(0.1619), tensor(0.1576), tensor(0.1535), tensor(0.1496), tensor(0.1459), tensor(0.1424), tensor(0.1390), tensor(0.1358), tensor(0.1327), tensor(0.1298), tensor(0.1270), tensor(0.1243), tensor(0.1217), tensor(0.1192), tensor(0.1169), tensor(0.1146), tensor(0.1124), tensor(0.1103), tensor(0.1083), tensor(0.1063), tensor(0.1044), tensor(0.1026), tensor(0.1008), tensor(0.0991), tensor(0.0975), tensor(0.0959), tensor(0.0944), tensor(0.0929), tensor(0.0914), tensor(0.0900), tensor(0.0887), tensor(0.0874), tensor(0.0861), tensor(0.0849), tensor(0.0837), tensor(0.0825), tensor(0.0814), tensor(0.0803), tensor(0.0792), tensor(0.0781), tensor(0.0771), tensor(0.0761), tensor(0.0752), tensor(0.0742), tensor(0.0733), tensor(0.0724), tensor(0.0715), tensor(0.0707), tensor(0.0699), tensor(0.0691), tensor(0.0683), tensor(0.0675), tensor(0.0667), tensor(0.0660), tensor(0.0653), tensor(0.0646), tensor(0.0639), tensor(0.0632), tensor(0.0626), tensor(0.0619), tensor(0.0613), tensor(0.0607), tensor(0.0600), tensor(0.0595), tensor(0.0589), tensor(0.0583), tensor(0.0577), tensor(0.0572), tensor(0.0567), tensor(0.0561), tensor(0.0556), tensor(0.0551), tensor(0.0546), tensor(0.0541), tensor(0.0536), tensor(0.0532), tensor(0.0527), tensor(0.0522), tensor(0.0518), tensor(0.0514), tensor(0.0509), tensor(0.0505), tensor(0.0501), tensor(0.0497), tensor(0.0493), tensor(0.0489), tensor(0.0485), tensor(0.0481), tensor(0.0477), tensor(0.0474), tensor(0.0470), tensor(0.0466), tensor(0.0463), tensor(0.0459), tensor(0.0456), tensor(0.0453), tensor(0.0449), tensor(0.0446), tensor(0.0443), tensor(0.0440), tensor(0.0437), tensor(0.0433), tensor(0.0430), tensor(0.0427), tensor(0.0424), tensor(0.0422), tensor(0.0419), tensor(0.0416), tensor(0.0413), tensor(0.0410), tensor(0.0408), tensor(0.0405), tensor(0.0402), tensor(0.0400), tensor(0.0397), tensor(0.0395), tensor(0.0392), tensor(0.0390), tensor(0.0387), tensor(0.0385), tensor(0.0382), tensor(0.0380), tensor(0.0378), tensor(0.0376), tensor(0.0373), tensor(0.0371), tensor(0.0369), tensor(0.0367), tensor(0.0365), tensor(0.0362), tensor(0.0360), tensor(0.0358), tensor(0.0356), tensor(0.0354), tensor(0.0352), tensor(0.0350), tensor(0.0348), tensor(0.0346), tensor(0.0344), tensor(0.0343), tensor(0.0341), tensor(0.0339), tensor(0.0337), tensor(0.0335), tensor(0.0333), tensor(0.0332), tensor(0.0330), tensor(0.0328), tensor(0.0327), tensor(0.0325), tensor(0.0323), tensor(0.0322), tensor(0.0320), tensor(0.0318), tensor(0.0317), tensor(0.0315), tensor(0.0314), tensor(0.0312), tensor(0.0310), tensor(0.0309), tensor(0.0307), tensor(0.0306), tensor(0.0305), tensor(0.0303), tensor(0.0302), tensor(0.0300), tensor(0.0299), tensor(0.0297), tensor(0.0296), tensor(0.0295), tensor(0.0293), tensor(0.0292), tensor(0.0291), tensor(0.0289), tensor(0.0288), tensor(0.0287), tensor(0.0285), tensor(0.0284), tensor(0.0283), tensor(0.0282), tensor(0.0280), tensor(0.0279), tensor(0.0278), tensor(0.0277), tensor(0.0276), tensor(0.0274), tensor(0.0273), tensor(0.0272), tensor(0.0271), tensor(0.0270), tensor(0.0269), tensor(0.0268), tensor(0.0266), tensor(0.0265), tensor(0.0264), tensor(0.0263), tensor(0.0262), tensor(0.0261), tensor(0.0260), tensor(0.0259), tensor(0.0258), tensor(0.0257), tensor(0.0256), tensor(0.0255), tensor(0.0254), tensor(0.0253), tensor(0.0252), tensor(0.0251), tensor(0.0250), tensor(0.0249), tensor(0.0248), tensor(0.0247), tensor(0.0246), tensor(0.0245), tensor(0.0244), tensor(0.0243), tensor(0.0242), tensor(0.0242), tensor(0.0241), tensor(0.0240), tensor(0.0239), tensor(0.0238), tensor(0.0237), tensor(0.0236), tensor(0.0235), tensor(0.0235), tensor(0.0234), tensor(0.0233), tensor(0.0232), tensor(0.0231), tensor(0.0230), tensor(0.0230), tensor(0.0229), tensor(0.0228), tensor(0.0227), tensor(0.0226), tensor(0.0226), tensor(0.0225), tensor(0.0224), tensor(0.0223), tensor(0.0223), tensor(0.0222), tensor(0.0221), tensor(0.0220), tensor(0.0220), tensor(0.0219), tensor(0.0218), tensor(0.0217), tensor(0.0217), tensor(0.0216), tensor(0.0215), tensor(0.0215), tensor(0.0214), tensor(0.0213), tensor(0.0212), tensor(0.0212), tensor(0.0211), tensor(0.0210), tensor(0.0210)]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>Text(0.5, 1.0, 'Model Loss Over Iterations')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-8-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The good news is that our model is able to cleanly partition the two points and loss is decreasing over time. This indicates that our model is working!</p>
</section>
<section id="benefits-of-momentum" class="level3">
<h3 class="anchored" data-anchor-id="benefits-of-momentum">Benefits of Momentum</h3>
<p>For small data sets, rates of convergence it not a big issues. However, for <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> large (think billions) we would like to things to go fast. This is where the momentum term comes into play. Intuitvely, if two consecutive points both have large negative gradients, we likely need to increase our step size. If the converse is true, we ought to reduce it.</p>
<p>Again, we’ll let <span class="math inline">\(n = 100\)</span>. To make things harder, we will set <span class="math inline">\(p = 10\)</span> and increase to <span class="math inline">\(\gamma = .25\)</span>. We will also increase the <span class="math inline">\(\epsilon\)</span>-tolerence to <span class="math inline">\(.1\)</span> for easier visualization. For Vanilla Gradient Descent, we again take <span class="math inline">\(\beta = 0\)</span> and change it to <span class="math inline">\(\beta = .75\)</span> for Gradient Descent with Momentum.</p>
<div id="cell-20" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Define parameters and Testing Data</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">.25</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> Utility.classification_data(n, gamma, p)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Initialize Model and train Vanilla Gradient Descent</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LogisticRegression()</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> Utility.train_logistic(X, y, alpha, beta, epsilon, LR)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">#Reset and Train Gradient Descent with Momentum</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>LR.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">.75</span> <span class="co">#increase beta term</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>res_momentum <span class="op">=</span> Utility.train_logistic(X, y, alpha, beta, epsilon, LR)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co">#plotting</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>[num_iters, loss] <span class="op">=</span> res</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>[num_iters_momentum, loss_momentum] <span class="op">=</span> res_momentum</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>iters <span class="op">=</span> torch.arange(<span class="dv">0</span>, num_iters)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>iters_momentum <span class="op">=</span> torch.arange(<span class="dv">0</span>, num_iters_momentum)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>plt.plot(iters, loss, color <span class="op">=</span> <span class="st">"red"</span>, label <span class="op">=</span> <span class="st">"Vanilla"</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>plt.plot(iters_momentum, loss_momentum, color <span class="op">=</span> <span class="st">"Blue"</span>, label <span class="op">=</span> <span class="st">"Momentum"</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model Loss Over Iterations"</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This is a great demonstration. Not only does Momentum Gradient Descent converge in 40 iterations faster than Vanilla Gradient Descent, but we also see how the momentum term causes the algorithm to overestimate weights before converging to a more optimal solution!</p>
</section>
<section id="overfitting" class="level3">
<h3 class="anchored" data-anchor-id="overfitting">Overfitting</h3>
<p>Finally, we test overfitting. In machine learning, overfitting occurs when we choose features the produce a high degree of accuracy in the training data, but don’t generalize to testing data well. In our experiment, we can model this by letting the number of features be greater than the number of observation. We take <span class="math inline">\(n = 15\)</span> and <span class="math inline">\(p = 30\)</span> to improve the speed at which we can find a high accuracy solution. Here, we generate some test set which is unknown to the model. Then, we find a training set that enables the model to achieve 100% accuracy. Finally we extract the weights from the trained model, and apply them to test set to how well they generialize:</p>
<div id="cell-23" class="cell" data-execution_count="149">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Define parameters, instantiate model</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">.25</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.01</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>LR <span class="op">=</span> LogisticRegression()</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co">#generate training and testing data</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>X_test, y_test <span class="op">=</span> Utility.classification_data(n, gamma, p)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">#Generate new training sets until 100% accuracy is achieved</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    X_train, y_train <span class="op">=</span> Utility.classification_data(n, gamma, p)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> Utility.train_logistic_with_test(X_train, y_train, X_test, y_test, alpha, beta, epsilon, LR)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> LR.predict(X_train)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    matching_count <span class="op">=</span> torch.<span class="bu">sum</span>(y_train <span class="op">==</span> pred).item()</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    percentage_match <span class="op">=</span> (matching_count <span class="op">/</span> <span class="bu">len</span>(pred))</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (percentage_match <span class="op">==</span> <span class="dv">1</span>):</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        LR.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="co">#Printing Accuracy Results</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training Accuracy: "</span> <span class="op">+</span> <span class="bu">str</span>(percentage_match))</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> (torch.matmul(X_test, LR.w) <span class="op">&gt;</span> <span class="dv">0</span>).<span class="bu">int</span>()</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>matching_count <span class="op">=</span> torch.<span class="bu">sum</span>(y_test <span class="op">==</span> pred).item()</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>percentage_match <span class="op">=</span> (matching_count <span class="op">/</span> <span class="bu">len</span>(pred))</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Testing Accuracy: "</span> <span class="op">+</span> <span class="bu">str</span>(percentage_match))</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a><span class="co">#Loss over Iterations</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>[num_iters, train_loss, test_loss] <span class="op">=</span> res</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>iters <span class="op">=</span> torch.arange(<span class="dv">0</span>, num_iters)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>plt.plot(iters, train_loss, color <span class="op">=</span> <span class="st">"blue"</span>, label <span class="op">=</span> <span class="st">"Training Loss"</span>)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>plt.plot(iters, test_loss, color <span class="op">=</span> <span class="st">"red"</span>, label <span class="op">=</span> <span class="st">"Testing Loss"</span>)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model Loss Over Iterations on Training and Testing Sets"</span>)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Accuracy: 1.0
Testing Accuracy: 0.8666666666666667</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-10-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Unsurprisingly, although our model has a training accuracy of 100%, the testing set only had a accuracy of 86%. Moreover, we see that as the model trains in the training set, it it does not reduce loss as dramatically on the testing set.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="media/descent.jpg" class="img-fluid figure-img"></p>
<figcaption>Gradient Descent in <span class="math inline">\(\mathbb{R}^2\)</span></figcaption>
</figure>
</div>
<p>Today we gave a high level overview of some of the mathematics the inform gradient descent. We used these ideas to implement gradient descent with Logistic Loss in PyTorch, discussing some of the components critical to the algorithm. Finally, we performed prooft-of-concept tests to demonstrate the mechanics behind regression models in the real world. Besides an intuitive understanding of gradient descent and loss minimization, there are two key observations that the reader should walk away with. First, gradient descent with momentum can speed up convergence, but may come at the cost of precision. As we saw in the second experiment, although Momentum Gradient Descent converged quicker, it first had to diverge. This can mean that It may “skip” or “bounce” over solutions more often than Vanilla Gradient Descent. Second, is that using too many features comes at the cost of algorithm speed and generializability. As we saw in the third experiment, although a large number of features may produce a high degree of accuracy on the training set, that predictive power does not generialize well. This truely is an instance of quality over quantity.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>