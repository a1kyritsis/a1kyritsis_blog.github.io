<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.551">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alec Kyritsis">
<meta name="dcterms.date" content="2024-04-25">
<meta name="description" content="Implementing the Adam Algorithm for Stochastic Gradient Descent in PyTorch">

<title>My Awesome CSCI 0451 Blog - ADAM Algorithm for Stochastic Gradient Descent</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: white;
      }

      .quarto-title-block .quarto-title-banner {
        color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
      }
</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">My Awesome CSCI 0451 Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">ADAM Algorithm for Stochastic Gradient Descent</h1>
                  <div>
        <div class="description">
          Implementing the Adam Algorithm for Stochastic Gradient Descent in PyTorch
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alec Kyritsis </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 25, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div id="cell-1" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Packages and Style</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statistics</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> adam_algorithm <span class="im">import</span> ADAM</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> adam_algorithm <span class="im">import</span> StochasticDescent</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> adam_algorithm <span class="im">import</span> Utility</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> adam_algorithm <span class="im">import</span> Penguins</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> newton <span class="im">import</span> LogisticRegression</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> newton <span class="im">import</span> Utility <span class="im">as</span> Util</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> newton <span class="im">import</span> Circle</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'font.family'</span>] <span class="op">=</span> <span class="st">"Courier"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre>
</div>
</div>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>Stochastic Gradient Descent methods offer notable speed-ups in classification tasks. We implement the ADAM Algorithm, and test performance on randomly generated and real datasets against Gradient Descent and a Stochastic Version, “Stochastic Descent”. We find ADAM converges faster than both methods across all experiments, and usually takes fewer iterations. We also emperically determine some good batch sizes. The code may be found at <a href="https://github.com/a1kyritsis/a1kyritsis_blog.github.io/tree/main/posts/AdamAlgorithm" class="uri">https://github.com/a1kyritsis/a1kyritsis_blog.github.io/tree/main/posts/AdamAlgorithm</a>.</p>
<p><img src="media/adamGuy.jpg" class="img-fluid"></p>
</section>
<section id="adam-an-algorithm-for-stochastic-gradient-descent" class="level1">
<h1>ADAM: An Algorithm for Stochastic Gradient Descent</h1>
<p>The advent of deep learning has arguably been driven by two conceptual breakthroughs: Automatic Differentiation and Stochastic Optimization. Today, we will explore the latter by investigating the seminal Adam Algorith. Adam was first proposed by Kingma and Ba in 2015, and reflects the need for memory and time effecient methods for convex optimization on large feature sets.</p>
<p>Methods like Logistic Regression and Newton’s Method operate on the entire data. The ingenious trick of stochastic optimization is to treat the data set as a sampling distribution for the gradient. Specifically, rather than calculate the gradient for all data points, at each iteration of descent Adam samples a subset of observations from the data set and updates the weight vector according to these points. If <span class="math inline">\(k\)</span> observations are sampled from a matrix of <span class="math inline">\(p\)</span> features, notice that any calculations W.R.T. optimization are performed on a <span class="math inline">\(k \times p\)</span> matrix. This greatly improves speed of computation and can prevent bottlenecks that arise to limited RAM and cache sizes relative to the data. Moreover, Adam and similar algorithms can facilitate convergence on noisy data sets by reducing the number of observations under consideration at each time step. The full paper, including pseudo code, may be found at <a href="https://arxiv.org/pdf/1412.6980.pdf%5D" class="uri">https://arxiv.org/pdf/1412.6980.pdf%5D</a> . Today, we give a high level overview of the algorithm and focus on some points of interest.</p>
<section id="algorithm-overview" class="level2">
<h2 class="anchored" data-anchor-id="algorithm-overview">Algorithm Overview</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="media/adamAlgo.jpg" class="img-fluid figure-img"></p>
<figcaption>Adam Algorith Pseudo Code</figcaption>
</figure>
</div>
<p>Adam may be thought of as a stochasitc framework through which run gradient-based optimization algorithms. Adam approximates the gradient using moving averages and variances. These correspond to the first and second moments respectively, or mean and uncentered variance. Let <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> be two hyperparameters to the model, <span class="math inline">\(\vec{w_t}\)</span> the set of weights, and <span class="math inline">\(g_t = \nabla_{\vec{w}} f_t (\vec{w}_{t - 1})\)</span> the gradient at time stamp <span class="math inline">\(t\)</span>. The emperical mean at step <span class="math inline">\(t\)</span> is calculated as</p>
<p><span class="math display">\[ m_t = \beta_1 \cdot m_{t - 1} +  (1 - \beta_1) \cdot g_t \]</span></p>
<p>and the uncentered variance</p>
<p><span class="math display">\[ v_t = \beta_2 \cdot v_{t - 1} +  (1 - \beta_2) \cdot g_t^2. \]</span></p>
<p>Notice that the updates are a weighting of these previous statistics along with the new gradient. Since the Adam Algorithm initializes <span class="math inline">\(m_0 = v_0 = (0, ..., 0)^p\)</span> and samples from the true gradient, there is associated bias. We can compute the bias corrected mean and variance as <span class="math display">\[\begin{align*}
    &amp;\hat{m}_t = \frac{m_t}{1 - \beta_1^t} \\
    &amp;\hat{v}_t = \frac{v_t}{1 - \beta_2^t} \\
\end{align*}\]</span></p>
<p>which we will explore shortly. Adam calculates both mean and variance to establish a trust region around each time step. Let <span class="math inline">\(\alpha\)</span> be the learning rate. Then the weight update is given by</p>
<p><span class="math display">\[ \vec{w}_t \longleftarrow \vec{w}_{t - 1} - \alpha \cdot \hat{m}_t / (\sqrt{\hat{v}_t} + \tau). \]</span></p>
<p>We may think of <span class="math inline">\(\frac{\hat{m_t}}{\sqrt{\hat{v}_t}}\)</span> as a signal to noise ratio. Intuitvely, the mean in the numerator approximates the gradient at time step <span class="math inline">\(t\)</span>. If the gradient is steeper, we expect our step to be larger whereas the step will be smaller if the converse is true. The variance provides a notion of accuracy W.R.T. the mean. Noisy data will produce a higher variances, which in turn results in a smaller step size. The converse is true when the data spread is tight.</p>
</section>
<section id="error-corrected-moments" class="level2">
<h2 class="anchored" data-anchor-id="error-corrected-moments">Error-Corrected Moments</h2>
<p>A central component of the Adam Algorithm is the error corection of the first and second moments. For completeness (and learning!), we derive the bias of the first moment.</p>
<p>Let <span class="math inline">\(g_1, ..., g_T\)</span> be the gradient at subsequent timesteps each sampled from uderlying distributions <span class="math inline">\(g_t \sim p(g_t)\)</span>. Suppose also that <span class="math inline">\(\mathbb{E}[g_t] = \mu_t\)</span>. Notice that the sample moment <span class="math inline">\(m_t\)</span> may be expressed as the sum of previous moments, namely</p>
<p><span class="math display">\[ m_t = (1 - \beta_2) \sum_{i = 1}^t \beta_1^{t - i} g_i.\]</span></p>
<p>Further notice that</p>
<p><span class="math display">\[\mathbb{E}[g_i] \sim \mu_t + \epsilon_i \]</span></p>
<p>where <span class="math inline">\(\epsilon_i\)</span> is some error. This comes from the fact that … We see to find the bias in <span class="math inline">\(m_t\)</span> with respect to the first W.R.T. the true first moment <span class="math inline">\(\mu\)</span>. Taking the expectation we have</p>
<p><span class="math display">\[ \mathbb{E}[m_t] = \mathbb{E} \left[ (1 - \beta_1) \sum_{i = 1}^t \beta_1^{t - i} g_i \right].\]</span></p>
<p>Apply linearity of expectation to push the expectation through the coefficients and sum</p>
<p><span class="math display">\[ \mathbb{E}[m_t] = (1 - \beta)\sum_{i = 1}^t \beta_1^{t - 1} \mathbb{E}[g_i].\]</span></p>
<p>Make the requisite substitution and do some algebra to see</p>
<p><span class="math display">\[\begin{align*}

    \mathbb{E}[m_1] &amp;= (1 - \beta_1) \sum_{i = 1}^t \beta_1^{t - i} (\mu_t + \epsilon_i) \\
                    &amp;= (1 - \beta_1) \left( \sum_{i = 1}^t \beta_1^{t - i}\mu_t + \sum_{i = 1}^t \beta_1^{t - i} \epsilon_i \right) \\
                    &amp;= (1 - \beta_1) \sum_{i = 1}^t \beta_2^{t - i} \mu_t +  (1 - \beta_1)\sum_{i = 1}^t \beta_1^{t - i} \epsilon_i \\
                    &amp;= \mu_t (1 - \beta_1) \sum_{i = 1}^t \beta_1^{t - i} +  (1 - \beta_1)\sum_{i = 1}^t \beta_1^{t - i} \epsilon_i.
\end{align*}\]</span></p>
<p>Notice that the right most term is independent of <span class="math inline">\(\mu\)</span> and hence my be absorbed as an error term <span class="math inline">\(\zeta\)</span>. We then have</p>
<p><span class="math display">\[\mu_t(1 - \beta_1) \sum_{i = 1}^t \beta_1^{t - i} + \zeta. \]</span></p>
<p>Next, we’ll get the geometric series out of the closet. Recall the sum of a finite gemoetric series is given by <span class="math inline">\(\sum_{k = 0}^{n - 1} a x^k = a \frac{1 - x^n}{1 - x}\)</span> and notice that</p>
<p><span class="math display">\[ \sum_{i = 1}^t \beta_1^{t - i} = \sum_{i = 0}^{t - 1} \beta_1^{i} = \frac{1 - \beta_1^t}{1 - \beta_1}. \]</span></p>
<p>Then we see that</p>
<p><span class="math display">\[\begin{align*}
    \mu_t (1 - \beta_1) \sum_{i = 1}^t \beta_1^{t - i} + \zeta &amp;= \mu_t (1 - \beta_1) \left(\frac{1 - \beta_1^t}{1 - \beta_1}\right) + \zeta \\
                                                            &amp;= \mu_t(1 - \beta_1^t) + \zeta
\end{align*}\]</span></p>
<p>and the desired form is produced. The bias here is <span class="math inline">\((1 - \beta_1^t)\)</span>. Since these quantities are all known, we may correct for this by deviding them out to produce the error corrected first moment</p>
<p><span class="math display">\[ \hat{m}_t = \frac{m_t}{1 - \beta_1^t}. \]</span></p>
<p>Similar logic may be applied to the second sample moment, <span class="math inline">\(v_t\)</span> to produce</p>
<p><span class="math display">\[ \hat{v}_t = \frac{v_t}{1 - \beta_2^t} \]</span></p>
<p>the error corrected raw second moment. The takeaway here is that sampling may produce error in our gradient calculations. On average we may correct this using constant parameters to our algorithm.</p>
</section>
</section>
<section id="implementation" class="level1">
<h1>Implementation</h1>
<p>One benefit of the Adam Algorithm is that it has a trivial implentation. We give one here using PyTorch:</p>
<div id="cell-8" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adam_optimizer(<span class="va">self</span>, X, y, k, alpha, beta_1, beta_2, tau):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>       <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">       Performs Stochastic Optimization on the objective</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">       function using the Adam Algorithm outlined by</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">       Kingma and Ba, 2015.</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">       Here:</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">           k := batch size</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">           alpha := step size for learning rate</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">           beta_1, beta_2 := expodential decay rates</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">           tau := small error corretion</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">       """</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>       <span class="cf">if</span> <span class="va">self</span>.v_t <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>           <span class="va">self</span>.v_t <span class="op">=</span> torch.zeros(X.shape[<span class="dv">1</span>])</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>       <span class="cf">if</span> <span class="va">self</span>.m_t <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>           <span class="va">self</span>.m_t <span class="op">=</span> torch.zeros(X.shape[<span class="dv">1</span>])</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>       <span class="cf">if</span> <span class="va">self</span>.w <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>           <span class="va">self</span>.w <span class="op">=</span> torch.rand((X.size()[<span class="dv">1</span>]))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>       n <span class="op">=</span> X.shape[<span class="dv">0</span>] <span class="co"># get number of features</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>       indices <span class="op">=</span> torch.randperm(n)[[torch.arange(k)]] <span class="co"># select k random indices</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>       X_mini <span class="op">=</span> X[indices, :] <span class="co"># sample points</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>       y_mini <span class="op">=</span> y[[indices]] <span class="co"># corresponding labels</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>       <span class="va">self</span>.t <span class="op">+=</span> <span class="dv">1</span> <span class="co"># update the time stamp</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>       g_t <span class="op">=</span> <span class="va">self</span>.grad(X_mini, y_mini, <span class="va">self</span>.score(X_mini)) <span class="co"># find the mini batch gradient</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>       m_t <span class="op">=</span> beta_1 <span class="op">*</span> <span class="va">self</span>.m_t <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta_1) <span class="op">*</span> g_t <span class="co"># find the first moment</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>       v_t <span class="op">=</span> beta_2 <span class="op">*</span> <span class="va">self</span>.v_t <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta_2) <span class="op">*</span> g_t<span class="op">**</span><span class="dv">2</span> <span class="co"># find the second moment</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>       m_hat <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta_1<span class="op">**</span><span class="va">self</span>.t) <span class="op">*</span> m_t <span class="co"># error corrected first moment</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>       v_hat <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta_2<span class="op">**</span><span class="va">self</span>.t) <span class="op">*</span> v_t <span class="co"># error corrected second moment</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>       <span class="va">self</span>.w <span class="op">=</span> <span class="va">self</span>.w <span class="op">-</span> (alpha <span class="op">*</span> m_hat <span class="op">/</span> (torch.sqrt(v_hat) <span class="op">+</span> tau)) <span class="co"># update weights</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The hyper parameters are identical to the ones defined above. <span class="math inline">\(\texttt{self.grad}\)</span> may be the gradient calculation of an arbitrary objective function. In this case, we’ll use Logistic Regression. The critical lines of code are the the fives lines following the <span class="math inline">\(\texttt{if}\)</span> blocks. This generates a random perutation of <span class="math inline">\(n\)</span> integers, and selects the first <span class="math inline">\(k\)</span> as the indices to be used in the gradient approximation. <span class="math inline">\(\texttt{X\_mini}\)</span> is the <span class="math inline">\(k \times p\)</span> sample matrix and <span class="math inline">\(\texttt{y\_mini}\)</span> the <span class="math inline">\(1 \times k\)</span> vector of corresponding labels. Other than these facts, the code is in correspondence with the pseudo code outlined in the paper.</p>
</section>
<section id="experiments" class="level1">
<h1>Experiments</h1>
<p>As per usual, we’ll test Adam on an easy data set to make sure things are working. We’ll let the number of observations <span class="math inline">\(n = 50\)</span>. The noise will be a bit higher than normal to produce some variation in the loss over time. For easy visualiation, we’ll take the number of features <span class="math inline">\(p = 2\)</span>. We’ll use the parameters suggested in the paper (<span class="math inline">\(\beta_1 .9, \beta_2 = .999, \tau = 10^{-8}\)</span>) with the exception the <span class="math inline">\(\alpha\)</span>, which we’ll set to <span class="math inline">\(.8\)</span> for fast convergence. Finally, we’ll use the norm of the gradient as a stopping condition with tolerence <span class="math inline">\(\epsilon = .01\)</span> and a generious batch size of <span class="math inline">\(10\)</span>.</p>
<div id="cell-11" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Test Parameters and data generation</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">.3</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> Utility.classification_data(n, gamma, p)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Adam init and parameters</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>adam <span class="op">=</span> ADAM()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>beta_2 <span class="op">=</span> <span class="fl">.999</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.01</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">#Train the model and extract the weights</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> Utility.train_adam(X, y, batch_size, alpha, beta_1, beta_2, tau, epsilon, adam)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> adam.w</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">#plotting decision boundary</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:,<span class="dv">0</span>], X[:, <span class="dv">1</span>], c <span class="op">=</span> y, cmap <span class="op">=</span> <span class="st">"coolwarm"</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>Utility.draw_line(w, <span class="op">-</span><span class="dv">2</span>, <span class="fl">2.5</span>, plt, color <span class="op">=</span> <span class="st">"red"</span>, linestyle <span class="op">=</span> <span class="st">"dashed"</span>, label <span class="op">=</span> <span class="st">"Decision Boundary"</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Feature 0"</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Feature 1"</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Test Points"</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co">#plotting loss over iterations</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>[num_iters, loss] <span class="op">=</span> res</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> torch.arange(<span class="dv">0</span>, num_iters)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>plt.plot(i, loss, color <span class="op">=</span> <span class="st">"red"</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Model Loss Over Iterations"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[tensor(0.3256), tensor(0.2145), tensor(0.1533), tensor(0.1152), tensor(0.0909), tensor(0.0832), tensor(0.0864), tensor(0.0829), tensor(0.0862), tensor(0.1157), tensor(0.0863), tensor(0.0697), tensor(0.0955), tensor(0.0742), tensor(0.0568), tensor(0.0519), tensor(0.0419), tensor(0.0470), tensor(0.0419), tensor(0.0419), tensor(0.0381), tensor(0.0343), tensor(0.0493), tensor(0.0342), tensor(0.0412), tensor(0.0312), tensor(0.0341), tensor(0.0274), tensor(0.0257), tensor(0.0276), tensor(0.0258), tensor(0.0277), tensor(0.0259), tensor(0.0278), tensor(0.0260), tensor(0.0211), tensor(0.0339), tensor(0.0233), tensor(0.0183)]
[tensor(0.3256), tensor(0.2145), tensor(0.1533), tensor(0.1152), tensor(0.0909), tensor(0.0832), tensor(0.0864), tensor(0.0829), tensor(0.0862), tensor(0.1157), tensor(0.0863), tensor(0.0697), tensor(0.0955), tensor(0.0742), tensor(0.0568), tensor(0.0519), tensor(0.0419), tensor(0.0470), tensor(0.0419), tensor(0.0419), tensor(0.0381), tensor(0.0343), tensor(0.0493), tensor(0.0342), tensor(0.0412), tensor(0.0312), tensor(0.0341), tensor(0.0274), tensor(0.0257), tensor(0.0276), tensor(0.0258), tensor(0.0277), tensor(0.0259), tensor(0.0278), tensor(0.0260), tensor(0.0211), tensor(0.0339), tensor(0.0233), tensor(0.0183)]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>Text(0.5, 1.0, 'Model Loss Over Iterations')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-4-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Using logistic regression, Adam is able to cleanly find the decsion boundary in just under 40 iterations. Turning to the second chart, we see that the emperical loss converges to just south of 0.05. One one clear difference between Adam and previous optimizers is that the loss is not monotonically decreasing (strictly decreasing). This is a directly product of Adam’s stochasticity: Some samples may not be good approximation of the gradient as a whole, causing Adam to mistep. Also notice that the relative magnitude of perturbations decreases over runtime duration. This is nice consequence of and Adam’s self-annealing error correction term and fact that the size of the gradient is smaller as Adam approaches convergence.</p>
<p>Next, we’ll explore how different batch sizes affects the number of iterations it takes Adam to converge. We’ll cut Adam off at 10,000 iterations and take <span class="math inline">\(n = 1000\)</span> and <span class="math inline">\(p = 50\)</span> to better approximate real use cases. We’ll keep all the hyperparamters the same except <span class="math inline">\(\alpha\)</span>, which we’ll set to <span class="math inline">\(\alpha = .5\)</span> to improve precision.</p>
<p>The test itself, <span class="math inline">\(\texttt{adam\_batch\_test}\)</span> runs Adam on each gamma values with each batch size. For each pair, it records the mean number of iterations to <span class="math inline">\(\epsilon\)</span>-convergence after 1000 trials.</p>
<div id="cell-13" class="cell" data-execution_count="253">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Test Parameters and data generation</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>MAX_ITERS <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Adam init and parameters</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>adam <span class="op">=</span> ADAM()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>batch_sizes <span class="op">=</span> [<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">150</span>, <span class="dv">300</span>, <span class="dv">500</span>, <span class="dv">750</span>, <span class="dv">900</span>]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>beta_2 <span class="op">=</span> <span class="fl">.999</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.01</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>trials <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>gamma_vals <span class="op">=</span> [<span class="fl">.12</span>, <span class="fl">.3</span>, <span class="fl">.5</span>, <span class="fl">.75</span>]</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> Utility.adam_batch_test(n, p, gamma_vals, batch_sizes, trials, [alpha, beta_1, beta_2, tau, epsilon, adam])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-14" class="cell" data-execution_count="254">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>plt.plot(batch_sizes, res[<span class="dv">0</span>], <span class="st">'o'</span>, color <span class="op">=</span> <span class="st">"#58508d"</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>plt.plot(batch_sizes, res[<span class="dv">0</span>], color <span class="op">=</span> <span class="st">"#58508d"</span>, label <span class="op">=</span> <span class="st">"gamma = .12"</span>) <span class="co"># #58508d #bc5090 #ff6361 #ffa600</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.plot(batch_sizes, res[<span class="dv">1</span>], <span class="st">'o'</span>, color <span class="op">=</span> <span class="st">"#bc5090"</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>plt.plot(batch_sizes, res[<span class="dv">1</span>], color <span class="op">=</span> <span class="st">"#bc5090"</span>, label <span class="op">=</span> <span class="st">"gamma = .30"</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.plot(batch_sizes, res[<span class="dv">2</span>], <span class="st">'o'</span>, color <span class="op">=</span> <span class="st">"#ff6361"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>plt.plot(batch_sizes, res[<span class="dv">2</span>], color <span class="op">=</span> <span class="st">"#ff6361"</span>, label <span class="op">=</span> <span class="st">"gamma = .50"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>plt.plot(batch_sizes, res[<span class="dv">3</span>], <span class="st">'o'</span>, color <span class="op">=</span> <span class="st">"#ffa600"</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>plt.plot(batch_sizes, res[<span class="dv">3</span>], color <span class="op">=</span> <span class="st">"#ffa600"</span>, label <span class="op">=</span> <span class="st">"gamma = .75"</span>) </span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Batch Size"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Mean Iterations to Convergence"</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Convergence by Batch Size"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here, the different curves correspond to level of noise in the data. Unsurpisingly, increasing the noise also increases the number of iterations to convergence accross all batch sizes. The curves are also U-tailed, with extreme batch size taking more iterations to converge. This makese sense, since fewer points can make it harder to approximate the gradient, whereas too many points leads to higher variances and thereby steps which are too small. The latter has a mutch more dramatic effect on convergence . Interestingly, the region middle region of the graph is quite wide. This suggests that one should err on the side of small to medium batch sizes with Adam, but there are many “correct” choices. This analysis agrees with best practice, which reccomends using the largest batch size which can fit in main memory.</p>
<p>Next, we’ll examine Adam’s performance when compared to Logistic Regression with Momentum along with its stochastic counterpart. For convenience let’s call it Stochastic Descent.</p>
<div id="cell-16" class="cell" data-execution_count="399">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Initialize Models</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>logistic <span class="op">=</span> LogisticRegression()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>stochastic <span class="op">=</span> StochasticDescent()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>adam <span class="op">=</span> ADAM()</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Define Testing Parameters and Generate Data</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">.25</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> Utility.classification_data(n, gamma, p)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">#logistic parameters</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>alpha_L <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>beta_L <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">#Stochastic Parameters</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>alpha_S <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>beta_S <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co">#Adam parameters</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>k_batch <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>beta_2 <span class="op">=</span> <span class="fl">.999</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Training Models</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>[L_step, L_loss, L_time] <span class="op">=</span> Util.train_logistic(X, y, alpha_L, beta_L, epsilon, logistic)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>[S_step, S_loss, S_time] <span class="op">=</span> Utility.train_stochastic(X, y, k_batch, alpha_S, beta_S, epsilon, stochastic)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>[A_step, A_loss, A_time] <span class="op">=</span> Utility.train_adam(X, y, k_batch, alpha, beta_1, beta_2, tau, epsilon, adam)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>plt.plot(torch.arange(<span class="dv">1</span>, L_step <span class="op">+</span> <span class="dv">1</span>), L_loss, label <span class="op">=</span> <span class="st">"Logistic Regression"</span>, color <span class="op">=</span> <span class="st">"green"</span>)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>plt.plot(torch.arange(<span class="dv">1</span>, S_step <span class="op">+</span> <span class="dv">1</span>), S_loss, label <span class="op">=</span> <span class="st">"Stochastic Descent"</span>, color <span class="op">=</span> <span class="st">"blue"</span>)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>plt.plot(torch.arange(<span class="dv">1</span>, A_step <span class="op">+</span> <span class="dv">1</span>), A_loss, label <span class="op">=</span> <span class="st">"ADAM"</span>, color <span class="op">=</span> <span class="st">"red"</span>)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Loss over Iterations"</span>)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Logistic Regression and Stochastic Descent have similar loss curves, with slightly higher loss over time from the stochastic algorithm. We see that Adam is able to converge in about two iterations quicker with less per-iteration loss than the other methods. The peaks are reflective of the momentum term. Interestingly, Adam’s step bound seemed to be able to counteract this, leading to smoother convergence.</p>
<p>While this experiment gave us a peak into the mechanics of these algorithms, to assess the performance of random algorithms on random data, we again need to look at averages over many trials. We’ll test the performance of these three algorithms on linearly-seperable data, noisy linearly-seperable data, and non-linear data. For each, we’ll assess average convergence by iteration count and run time, along with model accuracy. First, we’ll start with the non-noisy linearly seperable data. We’ll take <span class="math inline">\(\gamma = .25\)</span>, <span class="math inline">\(n = 1000\)</span>, <span class="math inline">\(p = 50\)</span>, and run each algorithm on 1000 different data sets.</p>
<div id="cell-18" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Initialize Models</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>logistic <span class="op">=</span> LogisticRegression()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>stochastic <span class="op">=</span> StochasticDescent()</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>adam <span class="op">=</span> ADAM()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Define Testing Parameters and Generate Data</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">.25</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.05</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>trials <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co">#logistic parameters</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>alpha_L <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>beta_L <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co">#Stochastic Parameters</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>alpha_S <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>beta_S <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co">#Adam parameters</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>k_batch <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>beta_2 <span class="op">=</span> <span class="fl">.999</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>L_iters, L_times, L_accuracies <span class="op">=</span> [], [], []</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>S_iters, S_times, S_accuracies <span class="op">=</span> [], [], []</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>A_iters, A_times, A_accuracies <span class="op">=</span> [], [], [] </span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> s <span class="op">&lt;</span> trials:</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> Utility.classification_data(n, gamma, p)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    [L_step, L_loss, L_time] <span class="op">=</span> Util.train_logistic(X, y, alpha_L, beta_L, epsilon, logistic)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    [S_step, S_loss, S_time] <span class="op">=</span> Utility.train_stochastic(X, y, k_batch, alpha_S, beta_S, epsilon, stochastic)</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    [A_step, A_loss, A_time] <span class="op">=</span> Utility.train_adam(X, y, k_batch, alpha, beta_1, beta_2, tau, epsilon, adam)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> ((math.isnan(logistic.loss(X, y))) <span class="kw">and</span> (math.isnan(stochastic.loss(X, y))) <span class="kw">and</span> (math.isnan(adam.loss(X, y)))):</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>        s <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(s)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>        L_iters.append(L_step)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>        L_times.append(L_time)</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>        L_accuracies.append(Utility.algo_accuracy(X, y, logistic))</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>        S_iters.append(S_step)</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>        S_times.append(S_time)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>        S_accuracies.append(Utility.algo_accuracy(X, y, stochastic))</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        A_iters.append(A_step)</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>        A_times.append(A_time)</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>        A_accuracies.append(Utility.algo_accuracy(X, y, adam))</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>    logistic.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    logistic.w_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    logistic.w_next <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>    stochastic.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>    stochastic.w_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>    stochastic.w_next <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>    stochastic.t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>    adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>    adam.w_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>    adam.w_next <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    adam.m_t <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>    adam.v_t <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>    adam.t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-19" class="cell" data-execution_count="402">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>algorithms <span class="op">=</span> [<span class="st">"Logistic Regression"</span>, <span class="st">"Stochastic Descent"</span>, <span class="st">"ADAM"</span>]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> [statistics.mean(L_iters), statistics.mean(S_iters), statistics.mean(A_iters)]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> [statistics.mean(L_times), statistics.mean(S_times), statistics.mean(A_times)]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>accuracies <span class="op">=</span> [statistics.mean(L_accuracies), statistics.mean(S_accuracies), statistics.mean(A_accuracies)]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, iterations, color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>, <span class="st">"#ffa600"</span>])</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Average Iterations"</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Average Iterations to Convergence over 1000 Trials"</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, times, color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>, <span class="st">"#ffa600"</span>])</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Average Runtime (s)"</span>)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Average Runtime to Convergence over 1000 Trials"</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, accuracies, color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>, <span class="st">"#ffa600"</span>])</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Average Predictive Accuracy"</span>)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Average Predictive Accuracy over 1000 Trials"</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(iterations)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(times)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(accuracies)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-9-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[17.071, 17.167, 11.194]
[0.03419111347198486, 0.030178627967834472, 0.02002998352050781]
[0.960957, 0.9611219999999999, 0.994735]</code></pre>
</div>
</div>
<p>Logistic Regression and Stochastic Descent took on average 17 iterations to converge and ran for about 0.03 seconds. The Stochastic Descent algorithm ran 0.004 seconds quicker, indicating that the batch sampling increased speed of computation. Adam beat both methods, taking on average 11 iterations to converge in 0.02 seconds. Adam also yielded higher predictive accuracy, labeling 99.4% of training observations correctly compared to 96.1% and 96.11% of Logistic Regression and Stochastic Descent respectively.</p>
<p>We’ll run the exact same test on noisy data (<span class="math inline">\(\gamma = .75\)</span>):</p>
<div id="cell-21" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Initialize Models</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>logistic <span class="op">=</span> LogisticRegression()</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>stochastic <span class="op">=</span> StochasticDescent()</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>adam <span class="op">=</span> ADAM()</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Define Testing Parameters and Generate Data</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">.75</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.05</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>trials <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co">#logistic parameters</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>alpha_L <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>beta_L <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="co">#Stochastic Parameters</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>alpha_S <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>beta_S <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="co">#Adam parameters</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>k_batch <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>beta_2 <span class="op">=</span> <span class="fl">.999</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>L_iters, L_times, L_accuracies <span class="op">=</span> [], [], []</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>S_iters, S_times, S_accuracies <span class="op">=</span> [], [], []</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>A_iters, A_times, A_accuracies <span class="op">=</span> [], [], [] </span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> s <span class="op">&lt;</span> trials:</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> Utility.classification_data(n, gamma, p)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>    [L_step, L_loss, L_time] <span class="op">=</span> Util.train_logistic(X, y, alpha_L, beta_L, epsilon, logistic)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>    [S_step, S_loss, S_time] <span class="op">=</span> Utility.train_stochastic(X, y, k_batch, alpha_S, beta_S, epsilon, stochastic)</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>    [A_step, A_loss, A_time] <span class="op">=</span> Utility.train_adam(X, y, k_batch, alpha, beta_1, beta_2, tau, epsilon, adam)</span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> ((math.isnan(logistic.loss(X, y))) <span class="kw">and</span> (math.isnan(stochastic.loss(X, y))) <span class="kw">and</span> (math.isnan(adam.loss(X, y)))):</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        s <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(s)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>        L_iters.append(L_step)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        L_times.append(L_time)</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>        L_accuracies.append(Utility.algo_accuracy(X, y, logistic))</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        S_iters.append(S_step)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        S_times.append(S_time)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>        S_accuracies.append(Utility.algo_accuracy(X, y, stochastic))</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>        A_iters.append(A_step)</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>        A_times.append(A_time)</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>        A_accuracies.append(Utility.algo_accuracy(X, y, adam))</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>    logistic.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>    logistic.w_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>    logistic.w_next <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>    stochastic.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>    stochastic.w_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>    stochastic.w_next <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>    stochastic.t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>    adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>    adam.w_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>    adam.w_next <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>    adam.m_t <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>    adam.v_t <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>    adam.t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-22" class="cell" data-execution_count="415">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>algorithms <span class="op">=</span> [<span class="st">"Logistic Regression"</span>, <span class="st">"Stochastic Descent"</span>, <span class="st">"ADAM"</span>]</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> [statistics.mean(L_iters), statistics.mean(S_iters), statistics.mean(A_iters)]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> [statistics.mean(L_times), statistics.mean(S_times), statistics.mean(A_times)]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>accuracies <span class="op">=</span> [statistics.mean(L_accuracies), statistics.mean(S_accuracies), statistics.mean(A_accuracies)]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, iterations, color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>, <span class="st">"#ffa600"</span>])</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Average Iterations"</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Average Iterations to Convergence over 1000 Trials"</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, times, color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>, <span class="st">"#ffa600"</span>])</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Average Runtime (s)"</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Average Runtime to Convergence over 1000 Trials"</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, accuracies, color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>, <span class="st">"#ffa600"</span>])</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Average Predictive Accuracy"</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Average Predictive Accuracy over 1000 Trials"</span>)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(iterations)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(times)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(accuracies)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-11-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[27.996, 31.635, 20.81]
[0.017142401218414306, 0.02117722225189209, 0.012424294471740722]
[0.9768209999999999, 0.976915, 0.986432]</code></pre>
</div>
</div>
<p>This increases the number iterations to converge and runtime accross all algorithms. Interestingly, Stochastic Descent runs slower than Logistic Regression in terms of iteration count and time; average 28.00 over .017 seconds and 31.64 over .021 seconds respectively. This suggests that the batch sampling method did not pay off on the noisy data. Again, ADAM beats out both methods with an average of 20.81 iterations, and also boasts 98.64% accuracy compared to 97.68% of Logistic Regression and 97.69 of Stochastic Descent.</p>
<p>Next, we’ll see how the algorithms perform when the data is non-linearly seperable. The data has shape</p>
<div id="cell-24" class="cell" data-execution_count="406">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Input Parameters</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>r1 <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Non Linear Data</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>p1 <span class="op">=</span> Circle.generate_points(r1, n, gamma)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>p2 <span class="op">=</span> Circle.generate_points(r2, n, gamma)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.cat((p1, p2), dim <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>col <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> torch.ones(<span class="dv">2</span> <span class="op">*</span> n, <span class="dv">1</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.cat((X, col), dim <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> torch.zeros(<span class="dv">1</span>, n)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> torch.ones(<span class="dv">1</span>, n)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>y_circ <span class="op">=</span> torch.cat((y1, y2), dim <span class="op">=</span> <span class="dv">1</span>)[<span class="dv">0</span>, :]</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>X_circ <span class="op">=</span> X.<span class="bu">float</span>()</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting Non-Linear Data</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> Circle.plot_points([p1, p2])</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We’ll run our tests in the same way as above:</p>
<div id="cell-26" class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Initialize Models</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>logistic <span class="op">=</span> LogisticRegression()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>stochastic <span class="op">=</span> StochasticDescent()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>adam <span class="op">=</span> ADAM()</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Define Testing Parameters and Generate Data</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.05</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>trials <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">#logistic parameters</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>alpha_L <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>beta_L <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="co">#Stochastic Parameters</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>alpha_S <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>beta_S <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a><span class="co">#Adam parameters</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>k_batch <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>beta_2 <span class="op">=</span> <span class="fl">.999</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>L_iters, L_times, L_accuracies <span class="op">=</span> [], [], []</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>S_iters, S_times, S_accuracies <span class="op">=</span> [], [], []</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>A_iters, A_times, A_accuracies <span class="op">=</span> [], [], [] </span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> s <span class="op">&lt;</span> trials:</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    p1 <span class="op">=</span> Circle.generate_points(r1, n, gamma)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    p2 <span class="op">=</span> Circle.generate_points(r2, n, gamma)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> torch.cat((p1, p2), dim <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    col <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> torch.ones(<span class="dv">2</span> <span class="op">*</span> n, <span class="dv">1</span>)</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> torch.cat((X, col), dim <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>    y1 <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> torch.zeros(<span class="dv">1</span>, n)</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>    y2 <span class="op">=</span> <span class="fl">1.0</span> <span class="op">*</span> torch.ones(<span class="dv">1</span>, n)</span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>    y_circ <span class="op">=</span> torch.cat((y1, y2), dim <span class="op">=</span> <span class="dv">1</span>)[<span class="dv">0</span>, :]</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>    X_circ <span class="op">=</span> X.<span class="bu">float</span>()</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>    [L_step, L_loss, L_time] <span class="op">=</span> Util.train_logistic(X_circ, y_circ, alpha_L, beta_L, epsilon, logistic)</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>    [S_step, S_loss, S_time] <span class="op">=</span> Utility.train_stochastic(X_circ, y_circ, k_batch, alpha_S, beta_S, epsilon, stochastic)</span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>    [A_step, A_loss, A_time] <span class="op">=</span> Utility.train_adam(X_circ, y_circ, k_batch, alpha, beta_1, beta_2, tau, epsilon, adam)</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> ((math.isnan(logistic.loss(X_circ, y_circ))) <span class="kw">and</span> (math.isnan(stochastic.loss(X_circ, y_circ))) <span class="kw">and</span> (math.isnan(adam.loss(X_circ, y_circ)))):</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>        s <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(s)</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>        L_iters.append(L_step)</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>        L_times.append(L_time)</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>        L_accuracies.append(Utility.algo_accuracy(X_circ, y_circ, logistic))</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>        S_iters.append(S_step)</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>        S_times.append(S_time)</span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>        S_accuracies.append(Utility.algo_accuracy(X_circ, y, stochastic))</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>        A_iters.append(A_step)</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>        A_times.append(A_time)</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>        A_accuracies.append(Utility.algo_accuracy(X_circ, y_circ, adam))</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>    logistic.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>    logistic.w_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>    logistic.w_next <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>    stochastic.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>    stochastic.w_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>    stochastic.w_next <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>    stochastic.t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a>    adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a>    adam.w_prev <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a>    adam.w_next <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>    adam.m_t <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>    adam.v_t <span class="op">=</span> <span class="va">None</span></span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>    adam.t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-27" class="cell" data-execution_count="413">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>algorithms <span class="op">=</span> [<span class="st">"Logistic Regression"</span>, <span class="st">"Stochastic Descent"</span>, <span class="st">"ADAM"</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> [statistics.mean(L_iters), statistics.mean(S_iters), statistics.mean(A_iters)]</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>times <span class="op">=</span> [statistics.mean(L_times), statistics.mean(S_times), statistics.mean(A_times)]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>accuracies <span class="op">=</span> [statistics.mean(L_accuracies), statistics.mean(S_accuracies), statistics.mean(A_accuracies)]</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, iterations, color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>, <span class="st">"#ffa600"</span>])</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Average Iterations"</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Average Iterations to Convergence over 1000 Trials"</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, times, color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>, <span class="st">"#ffa600"</span>])</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Average Runtime (s)"</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Average Runtime to Convergence over 1000 Trials"</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, accuracies, color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>, <span class="st">"#ffa600"</span>])</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Average Predictive Accuracy"</span>)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Average Predictive Accuracy over 1000 Trials"</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(iterations)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(times)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(accuracies)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-14-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-14-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[32.873, 894.254, 976.287]
[0.014596573829650879, 0.5360904953479767, 0.4758875901699066]
[0.503959, 0.500758, 0.49998]</code></pre>
</div>
</div>
<p>Unsurprisingly, accuracy dropped lower accross all algorithms to around 50%. Both stochastic methods converged in around 900 iterations running for about half a second. Logistic Regression was the clear winer, taking just 32 iterations to converge in just 0.01 seconds. This indicates that without feature mapping, linear stochastic algorithms should not be applied to non-linear data sets, as the sampling approach fails to provide a holistic picture of the loss function.</p>
</section>
<section id="test-cases" class="level1">
<h1>Test Cases</h1>
<p>Next, we will examine ADAM’s performance vs a Stochastic instance of gradient descent on two real data sets.</p>
<section id="lucky-number-13" class="level2">
<h2 class="anchored" data-anchor-id="lucky-number-13">Lucky Number 13</h2>
<p>First, we will examine scikit-learn’s digits data set. The data set contains <span class="math inline">\(8 \times 8\)</span> pixel images of the handwritten digits <span class="math inline">\(0 - 9\)</span>, and is a commonly used benchmark for machine learning algorithms. We transform the data set into a torch tensor of floats. We can get a feel for the data by printing out some of the images as follows:</p>
<div id="cell-30" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> load_digits() <span class="co"># load the digits data from sckikit-learn</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> pd.DataFrame(data <span class="op">=</span> data.data, columns <span class="op">=</span> [<span class="ss">f"pixel_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(data.data.shape[<span class="dv">1</span>])]) <span class="co"># "flatten" the data such that each row corresponds to a digit, and columns are the pixels</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>digits[<span class="st">"target"</span>] <span class="op">=</span> data.target <span class="co"># asign correspond integer label</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows <span class="op">=</span> <span class="dv">3</span>, ncols <span class="op">=</span> <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> digit <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">10</span>):</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    digit_data <span class="op">=</span> digits[digits[<span class="st">"target"</span>] <span class="op">==</span> digit].head(<span class="dv">1</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    digit_image <span class="op">=</span> digit_data.iloc[<span class="dv">0</span>, :<span class="op">-</span><span class="dv">1</span>].values.reshape(<span class="dv">8</span>, <span class="dv">8</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes[(digit <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> <span class="dv">3</span>, (digit <span class="op">-</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">3</span>]</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    ax.imshow(digit_image, cmap <span class="op">=</span> <span class="st">"grey"</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"</span><span class="sc">{</span>digit<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">"off"</span>)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Notice that since the instances of Logistic Regression that we are using only support two label classification, we must choose two digits and assign a binary label to them. Here, we’ll take <span class="math inline">\(1\)</span> and <span class="math inline">\(3\)</span>:</p>
<div id="cell-32" class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># grab digits</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>two_digits <span class="op">=</span> digits[digits[<span class="st">"target"</span>].isin([<span class="dv">1</span>, <span class="dv">3</span>])] <span class="co"># only look at digits 1,3</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>two_digits[<span class="st">"label"</span>] <span class="op">=</span> (two_digits[<span class="st">"target"</span>] <span class="op">==</span> <span class="dv">1</span>).astype(<span class="bu">int</span>) <span class="co"># assign binary lable</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We also need to seperate the labels from the data. We do this as follows:</p>
<div id="cell-34" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># seperate labels from data</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.tensor(two_digits.drop(columns <span class="op">=</span> [<span class="st">"label"</span>, <span class="st">"target"</span>]).values, dtype <span class="op">=</span> <span class="bu">float</span>).<span class="bu">float</span>() <span class="co"># convert to torch tensor</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor(two_digits[<span class="st">"label"</span>].values, dtype <span class="op">=</span> <span class="bu">float</span>).<span class="bu">float</span>() <span class="co"># convert to torch tensor</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We are now ready to test the performance of the algorithms. Similar to our other tests, we’ll look at the number of epochs (iterations) to convergence, the time in seconds it takes the algorithms to converge, as well as the accuracy of each algorithm on the training set.</p>
<div id="cell-36" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># epsilon tolerance</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Initialize Models</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>stochastic <span class="op">=</span> StochasticDescent()</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>adam <span class="op">=</span> ADAM()</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Stochastic Parameters</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>alpha_S <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>beta_S <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Adam parameters</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>k_batch <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>beta_2 <span class="op">=</span> <span class="fl">.999</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Training Models</span></span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>[S_step, S_loss, S_time] <span class="op">=</span> Utility.train_stochastic(X, y, k_batch, alpha_S, beta_S, epsilon, stochastic)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>[A_step, A_loss, A_time] <span class="op">=</span> Utility.train_adam(X, y, k_batch, alpha, beta_1, beta_2, tau, epsilon, adam)</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>algorithms <span class="op">=</span> [<span class="st">"Stochastic Descent"</span>, <span class="st">"ADAM"</span>]</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting loss over epochs</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>plt.plot(torch.arange(<span class="dv">1</span>, S_step <span class="op">+</span> <span class="dv">1</span>), S_loss, label <span class="op">=</span> <span class="st">"Stochastic Descent"</span>, color <span class="op">=</span> <span class="st">"blue"</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>plt.plot(torch.arange(<span class="dv">1</span>, A_step <span class="op">+</span> <span class="dv">1</span>), A_loss, label <span class="op">=</span> <span class="st">"ADAM"</span>, color <span class="op">=</span> <span class="st">"red"</span>)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Loss over Iterations on Two Digits"</span>)</span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a><span class="co"># printing results</span></span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stochastic Descent converged in "</span> <span class="op">+</span> <span class="bu">str</span>(S_step) <span class="op">+</span> <span class="st">" epochs."</span>)</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ADAM converged in "</span> <span class="op">+</span> <span class="bu">str</span>(A_step) <span class="op">+</span> <span class="st">" epochs."</span>)</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting times</span></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, [S_time, A_time], color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>])</span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Time to Convergence (s)"</span>)</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Convergence Time on Two Digits"</span>)</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a><span class="co"># printing results</span></span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stochastic Descent took "</span> <span class="op">+</span> <span class="bu">str</span>(S_time) <span class="op">+</span> <span class="st">" seconds to converge."</span>)</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ADAM took "</span> <span class="op">+</span> <span class="bu">str</span>(A_time) <span class="op">+</span> <span class="st">" seconds to converge."</span>)</span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>[S_acc, A_acc] <span class="op">=</span> [Utility.algo_accuracy(X, y, stochastic), Utility.algo_accuracy(X, y, adam)]</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting accuracies</span></span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, [S_acc, A_acc], color <span class="op">=</span> [<span class="st">"#bc5090"</span>, <span class="st">"#ff6361"</span>])</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Predictive Accuracy"</span>)</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Accuracy on Two Digits"</span>)</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb23-52"><a href="#cb23-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb23-53"><a href="#cb23-53" aria-hidden="true" tabindex="-1"></a><span class="co"># printing results</span></span>
<span id="cb23-54"><a href="#cb23-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stochastic Descent had an accuracy of "</span> <span class="op">+</span> <span class="bu">str</span>(S_acc <span class="op">*</span> <span class="dv">100</span>) <span class="op">+</span> <span class="st">"%."</span>)</span>
<span id="cb23-55"><a href="#cb23-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ADAM had an accuracy of "</span> <span class="op">+</span> <span class="bu">str</span>(A_acc <span class="op">*</span> <span class="dv">100</span>) <span class="op">+</span> <span class="st">"%."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Stochastic Descent converged in 671 epochs.
ADAM converged in 259epochs.
Stochastic Descent took 0.790539026260376 seconds to converge.
ADAM took 0.26377224922180176 seconds to converge.
Stochastic Descent had an accuracy of 100.0%.
ADAM had an accuracy of 100.0%.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-18-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-18-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Both models achieved 100 % accuracy. ADAM was significantly quicker, converging in about .26 seconds compared to Stochastic Descent’s time of .79. Interestingly, Stochastic Descent descended quicker than ADAM, but struggled to converge to a small enough gradient. This phenomenon is likely a product of ADAM’s self-annealing error correction term, which decreases the magnitude of the step as the number of iterations increase.</p>
</section>
</section>
<section id="palmer-penguins" class="level1">
<h1>Palmer Penguins!</h1>
<p>Next, we’ll test the models on a familar dataset. The Palmer Penguins!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="media/penguin.jpg" class="img-fluid figure-img"></p>
<figcaption>Dat boi.</figcaption>
</figure>
</div>
<p>We’ll apply a similar procedure to the digits dataset: Load the data, convert features and labels to torch tensors, and choose two penguins to classify.</p>
<div id="cell-39" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>target_features <span class="op">=</span> [<span class="st">"Culmen Length (mm)"</span>, <span class="st">"Culmen Depth (mm)"</span>, <span class="st">"Flipper Length (mm)"</span>]</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> Penguins.loadTrainingData()</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>[penguins, labels] <span class="op">=</span> Penguins.prepareData(penguins)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>two_penguins <span class="op">=</span> penguins[penguins[<span class="st">"Species"</span>].isin([<span class="dv">0</span>, <span class="dv">1</span>])]</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor(two_penguins[<span class="st">"Species"</span>].values, dtype <span class="op">=</span> <span class="bu">float</span>).<span class="bu">float</span>()</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.tensor(two_penguins[target_features].values, dtype <span class="op">=</span> <span class="bu">float</span>).<span class="bu">float</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’ll run identical test, slightly tuning the testing parameters to fit the data.</p>
<div id="cell-41" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># epsilon tolerance</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Initialize Models</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>stochastic <span class="op">=</span> StochasticDescent()</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>adam <span class="op">=</span> ADAM()</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Stochastic Parameters</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>alpha_S <span class="op">=</span> <span class="fl">.8</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>beta_S <span class="op">=</span> <span class="fl">.5</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co">#Adam parameters</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>adam.w <span class="op">=</span> <span class="va">None</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>k_batch <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="op">=</span> <span class="fl">.9</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>beta_2 <span class="op">=</span> <span class="fl">.999</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="dv">8</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Training Models</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>[S_step, S_loss, S_time] <span class="op">=</span> Utility.train_stochastic(X, y, k_batch, alpha_S, beta_S, epsilon, stochastic)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>[A_step, A_loss, A_time] <span class="op">=</span> Utility.train_adam(X, y, k_batch, alpha, beta_1, beta_2, tau, epsilon, adam)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a><span class="co">#Plotting</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>algorithms <span class="op">=</span> [<span class="st">"Stochastic Descent"</span>, <span class="st">"ADAM"</span>]</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting loss over epochs</span></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>plt.plot(torch.arange(<span class="dv">1</span>, S_step <span class="op">+</span> <span class="dv">1</span>), S_loss, label <span class="op">=</span> <span class="st">"Stochastic Descent"</span>, color <span class="op">=</span> <span class="st">"blue"</span>)</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>plt.plot(torch.arange(<span class="dv">1</span>, A_step <span class="op">+</span> <span class="dv">1</span>), A_loss, label <span class="op">=</span> <span class="st">"ADAM"</span>, color <span class="op">=</span> <span class="st">"red"</span>)</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Loss over Iterations on Two Penguins"</span>)</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a><span class="co"># printing results</span></span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stochastic Descent converged in "</span> <span class="op">+</span> <span class="bu">str</span>(S_step) <span class="op">+</span> <span class="st">" epochs."</span>)</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ADAM converged in "</span> <span class="op">+</span> <span class="bu">str</span>(A_step) <span class="op">+</span> <span class="st">"epochs."</span>)</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting times</span></span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, [S_time, A_time], color <span class="op">=</span> [<span class="st">"#010c30"</span>, <span class="st">"#27a8f7"</span>])</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Time to Convergence (s)"</span>)</span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Convergence Time on Two Penguins"</span>)</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a><span class="co"># printing results</span></span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stochastic Descent took "</span> <span class="op">+</span> <span class="bu">str</span>(S_time) <span class="op">+</span> <span class="st">" seconds to converge."</span>)</span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ADAM took "</span> <span class="op">+</span> <span class="bu">str</span>(A_time) <span class="op">+</span> <span class="st">" seconds to converge."</span>)</span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>[S_acc, A_acc] <span class="op">=</span> [Utility.algo_accuracy(X, y, stochastic), Utility.algo_accuracy(X, y, adam)]</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting accuracies</span></span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>plt.bar(algorithms, [S_acc, A_acc], color <span class="op">=</span> [<span class="st">"#010c30"</span>, <span class="st">"#27a8f7"</span>])</span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Algorithm"</span>)</span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Predictive Accuracy"</span>)</span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Accuracy on Two Penguins"</span>)</span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a><span class="co"># printing results</span></span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Stochastic Descent had an accuracy of "</span> <span class="op">+</span> <span class="bu">str</span>(S_acc <span class="op">*</span> <span class="dv">100</span>) <span class="op">+</span> <span class="st">"%."</span>)</span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"ADAM had an accuracy of "</span> <span class="op">+</span> <span class="bu">str</span>(A_acc <span class="op">*</span> <span class="dv">100</span>) <span class="op">+</span> <span class="st">"%."</span>)</span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Stochastic Descent converged in 104 epochs.
ADAM converged in 118epochs.
Stochastic Descent took 0.26360511779785156 seconds to converge.
ADAM took 0.06348228454589844 seconds to converge.
Stochastic Descent had an accuracy of 99.72602739726028%.
ADAM had an accuracy of 99.45205479452055%.</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-20-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="notebook_files/figure-html/cell-20-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Again, both algorithms yield similar accuracies. Adam took 14 more epochs to converge when compared to Stochastic Descent (118 vs 104) but did so faster (0.063 s vs 0.26). Similar to the digits data set, ADAM had a smaller loss slope compared to Stochastastic Descent, despite the higher learning rate (<span class="math inline">\(\alpha = .9\)</span> and <span class="math inline">\(\alpha = .8\)</span> respectively). In both cases, the analysis suggests that while ADAM may occasional take more epochs to converge, it does in a more computationally effecient manner.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Today, we explored the ADAM algorithm for gradient descent. We investigated the notion of sampling-based descent methods, and derived some of the error correction terms used in the algorithm. We then tested the algorithm in choice experimental cases along with some real datasets. There are three main takeaways. First, aside from extreme values, batch size does not appear to have a significant affect on ADAM’s iteration to convergence. This suggests that the hardware should determine the size of the mini batch as oppose to the data itself. Second, is that ADAM still yields significant performance benefits on noisy data. Third, ADAM is fast. Accross all tests, ADAM outperformed it’s countparts in terms of speed. It is then no suprise the ADAM is when of the choice algorithms for modern machine learning tasks.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>