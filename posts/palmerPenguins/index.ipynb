{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Palmer Penguins!\n",
    "author: Alec Kyritsis\n",
    "date: '2023-04-02'\n",
    "image: \"media/penguinSmall.jpg\"\n",
    "description: \"Classifying Penguins\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/penguinSmall.jpg)\n",
    "\n",
    "## Abstract\n",
    "\n",
    "We investigate the Palmer Penguin data set. We investigate quantitative and qualitative features that partion the set by species. We confirm our results with an automated feature selection process, and test four different model accuracy and speed (Decision Trees, Random Forrests, K-Nearest Neighbors, Logistic Regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying the Palmer Penguins\n",
    "\n",
    "Today we are going to classify some penguins. According to BirdLife international, penguins are one of the most threatened groups of seabirds, with half of the 18 species list as either vulnerable or endagered. This means that classification can be an import aspect in assessing populating dynammics.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset\n",
    "\n",
    "We'll first start by exploring the data set. I am most curious how different features partition the penguins by species. This will lend some context later on when we train our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "::: {#loan-intent layout-ncol=2}\n",
    "\n",
    "![](media/bodyMass.jpg)\n",
    "\n",
    "![](media/flipperLength.jpg)\n",
    "\n",
    ":::\n",
    "\n",
    "Above are box and wisker plots for penguin body mass and flipper length subdevided by species. Interesting, both Chinstrap and Adelie Penguins have similar distriubtions of these features. However, weight and flipper length clearly distinguishes Gentoo penguins from their counterparts. Moreover, flipper length appears to have a tighter spread accross Gentoo and Adelie penguins. Speaking of penguins, here's the squad:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "::: {#loan-intent layout-ncol=3}\n",
    "\n",
    "![Chinstrap](media/chinstrap.jpg)\n",
    "\n",
    "![Gentoo](media/gentoo.jpg)\n",
    "\n",
    "![Adelie](media/adelie.jpg)\n",
    "\n",
    ":::\n",
    "\n",
    "As a certified penguin enjoyer, I also couldn't help but notice that these penguins appear to have different types of beaks. The two features that correspond to this information are Culmen Length and Culmen depth. If we expect the beaks to be a distingishing feature, we might expect some seperation in the data set when we plot the two against eachother.\n",
    "\n",
    "\n",
    "![](media/lengthVDepth.jpg)\n",
    "\n",
    "\n",
    "Wow! We are able to clearly distinguish each species of penguin by the ratio of their culmen length to culmen depth. This might suggest that among these three penguins within this region, beaks have adapted to suit each individuals speice's unique needs like hunting, preening, and defense. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also curious about the species demographics accross the three islands.  \n",
    "\n",
    "![](media/island.jpg)\n",
    "\n",
    "Interestingly, Adelie penguins live on all three islands, and are the only species residing on Torgersen island. Dream Island also contains Chinstrap Penguins, and Biscoe Island contains the entire Gentoo population. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we'll take a look at a pairplot for select quantiative features to see if there is anything else that might be useful:\n",
    "\n",
    "![](media/pairPlot.jpg)\n",
    "\n",
    "Along the main diagonal, we can see the distriubtion of the features themselves for the three different species populations. We see that each feature only seperates out certain species. For example Flipper Length has one distinct mound centered right, but the two other mounds are overlapping. Besides culmen length to culmen depth, two other noteworthy charts our culmen length to flipper length, and culmen length to body mass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "The intial exploration of the data suggestions that if we are clever about feature selection, we should be able to select features the cut the dataset by penguin species. To confirm this, we will use recursive features selection (REF). REF works by assigning a predictive socre to each feature. In this case, we'll use linear regression. It then eliminates low-scoring features. The process is repeated until the desired number of features is reached. We'll define our REF function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(df, target_column, n) :\n",
    "    #returns n most predictive features for target_column\n",
    "    x = df.drop(target_column, axis = 1)\n",
    "    y = df[target_column]\n",
    "    [x_train, x_test, y_train, y_test] = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "    LR = LinearRegression()\n",
    "    rfe = RFE(estimator = LR, n_features_to_select = n)\n",
    "    rfe.fit(x_train, y_train)\n",
    "    return x.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things interesting, we'll use two quantitative features and one qualitative feature. To do this, we'll subdevide the dataset by quantitative features and qualitative features, and pass the augmented dataframe to our select features_features functions. In each case, we'll look at the top 3 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Cuts\n",
    "quantitativeFeatures = [\"Species\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Flipper Length (mm)\", \"Body Mass (g)\",  \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n",
    "qualitativeFeatures = [\"Species\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\", \"Clutch Completion_No\", \"Clutch Completion_Yes\", \"Sex_FEMALE\", \"Sex_MALE\"]\n",
    "#Selecting Features\n",
    "quantFeatures = utility.select_features(penguins[quantitativeFeatures], \"Species\", 3)\n",
    "qualFeatures = utility.select_features(penguins[qualitativeFeatures], \"Species\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three best quantiative features are Culmen Length, Culmen Depth, and Delta 15 N. The three best qualitative features are Island_Biscoe, Island_Torgenrsen, and Clutch Completion. This confirms our suspicious in our initial analysis. For the quantitative features, we'll take Culmen Length and Culmen Depth. In the intial data cleaning, I actually partitioned Island into three seperate columns, so we'll just take Island as our single qualitative feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "![](media/robo.jpg)\n",
    "\n",
    "Given the nature of this data set, there's a high likelilhood we will be able to train a model with 100% accuracy. A better question to ask here is what types of models work well with the features that we choose. From our initial gander at the data, we saw that no single feature cleanly partitions the penguins by species. However, composite features like Culmen Length to Column depth appeared to have high predictive power. Regression might then be a good choice here; particulary, we'll take a look at logistic regression. \n",
    "\n",
    "Another important observation is that features can reduce the number of choices that the model has to make. For example, if a model is passed Torgersen Island, there is an 100% chance that it is looking at a Adelie penguin. Even if the model is passed Biscoe Island, it still narrows the number of choices down to two birds (Adelie and Gentoo). From here, other features can be used to classfy. This type of logic implicates decision trees and random forrests. We'll test these as well.\n",
    "\n",
    "We'll define our predictive features and train the models as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining features\n",
    "predictiveFeatures = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n",
    "#Initializing Models\n",
    "tree = DecisionTreeClassifier() #1\n",
    "RF = RandomForestClassifier() #2\n",
    "KN = KNeighborsClassifier() #3\n",
    "LR = LogisticRegression(max_iter = 1000000) #4\n",
    "models = [tree, RF, KN, LR]\n",
    "modelNames = [\"Decision Tree\", \"Random Forrest\", \"K - Nearest Neighbors\", \"Logistic Regress\"]\n",
    "a = []\n",
    "i = 0\n",
    "#Training and Testing\n",
    "for model in models:\n",
    "\n",
    "    model.fit(penguins[predictiveFeatures], targetFeature)\n",
    "    pred = model.predict(testingPenguins[predictiveFeatures])\n",
    "    accuracy = accuracy_score(pred, testTargetFeature)\n",
    "    print(modelNames[i] + \" Accuracy: \" + str(accuracy))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies on the testing set are as follows: The Decision Tree scored 98.5%, the Random Forrest 100%, K - Nearest Neighbors 98.5%, and Logistic Regress 100%. We can investigate the decision process of our model by taking a look at the decision boundaries and confusion matrix:\n",
    "\n",
    "\n",
    "::: {#tree-analysis layout-ncol=2}\n",
    "\n",
    "![Decision Boundaries](media/tree.jpg)\n",
    "\n",
    "![Confusion Matrix](media/CM-tree.jpg)\n",
    "\n",
    ":::\n",
    "\n",
    "The decision tree made one wrong classification, mistaking a Adelie penguin for a Gentoo. Examining the decision boundaries, we see that a majority of the penguins are easily partitioned into their respective species, with the exception for a few penguins which lie near the boundaries. Interesting, although Torgersen island only contained Gentoo penguins, the model seemed to expect that other species could live there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "::: {#loan-intent layout-ncol=2}\n",
    "\n",
    "![Decision Boundary](media/forrest.jpg)\n",
    "\n",
    "![Confusion Matrix](media/CM-RF.jpg)\n",
    "\n",
    ":::\n",
    "\n",
    "The Random Forrest model correctly classified all the penguins. Unlike the Decision Tree Model, the Random Forrest was able to find that certain islands only contain certain species of penguins. Boundaries are also more linear, distinctlly partitioning islands into two decision regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "::: {#loan-intent layout-ncol=2}\n",
    "\n",
    "![Decision Boundary](media/neighbors.jpg)\n",
    "\n",
    "![Confusion Matrix](media/CM-KN.jpg)\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "The K-Nearest Neighbors Model mistook a Gentoo penguin for a Chinstrap penguin. Similar to the Decision Tree, the model assumed that all species of penguins may be present on each island. K-Nearest Neights also has \"rough\" decision boundaries, which likely reflects the mechanics of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {#loan-intent layout-ncol=2}\n",
    "\n",
    "![Decision Boundary](media/LR.jpg)\n",
    "\n",
    "![Confusion Matrix](media/CM-LR.jpg)\n",
    "\n",
    ":::\n",
    "\n",
    "Logistic Regression was able to correctly classify all penguins. Out of all the models, Logistic Regression appears to have the cleanest decision boundaries, with no points close to boundary. Finally, it is worth remarking that K-Nearest Neighbors and Decision Tree were able to train and predict on the data the fastest. This was followed by Logistic Regression and then the Random Forrest. This means that Logistic Regression is the fastest, accurate model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Overall, our investigation suggests the a small number of features can produce high efficacy--If we are able to cleverly partition the data set. For toy data sets, automated methods such as recursive feature selection can aid in the feature selection process. Logistic Regression was the fastest, most accurate model. However, K-Nearest Neighbors and Decision Trees both produced a high degree of accuracy at about $\\frac{1}{6}^{th}$ the speed. This makes them good options for large data sets (think billions of penguins) where we are O.K. with sacrificing accuracy for a boost in computation complexity. Random Forrests--although by far the slowest method--correctly classified all the penguins. This suggets that in more complex data sets where multiple features may be regquired to accuriately classify, Random Forrests can be a robust classifcation method."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
